[{"abstract":"The size of embeddings generated by large language models can negatively affect system latency and model size in certain downstream practical applications (e.g. KNN search). In this work, we propose EmbedTextNet, a light add-on network that can be appended to an arbitrary language model to generate a compact embedding without requiring any changes in its architecture or training procedure. Specifically, we use a correlation penalty added to the weighted reconstruction loss that better captures the informative features in the text embeddings, which improves the efficiency of the language models. We evaluated EmbedTextNet on three different downstream tasks: text similarity, language modelling, and text retrieval. Empirical results on diverse benchmark datasets demonstrate the effectiveness and superiority of EmbedTextNet compared to state-of-art methodologies in recent works, especially in extremely low dimensional embedding sizes. The developed code for reproducibility is included in the supplementary material.","anthology_url":"https://aclanthology.org/2023.findings-acl.625","authors":["Dae Yon Hwang","Bilal Taha","Yaroslav Nechaev"],"category":"Findings","demo_url":null,"display_track":"Machine Learning for NLP","event_ids":["session-1_-machine-learning-for-nlp-(virtual-poster)","spotlight-session_-spotlight---metropolitan-centre-(spotlight)"],"id":"P1258","is_paper":true,"keywords":["representation learning","model compression methods"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.625.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77434/poster_document/280fbeb68b32fcf5c1475fea7712157c.pdf","preview_image":"https://assets.underline.io/lecture/77434/poster/968641458a34e92b5096e7593aefb875.jpg","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77434/slideshow/606333221cf40a7592eb718693d11af0.pdf","title":"EmbedTextNet: Dimension Reduction with Weighted Reconstruction and Correlation Losses for Efficient Text Embedding","tldr":"The size of embeddings generated by large language models can negatively affect system latency and model size in certain downstream practical applications (e.g. KNN search). In this work, we propose EmbedTextNet, a light add-on network that can be appended to an arbitrary language model to generate ...","track":"Machine Learning for NLP","underline_id":77434,"underline_url":"https://underline.io/events/395/posters/15200/poster/77434-propsegment-a-large-scale-corpus-for-proposition-level-segmentation-and-entailment-recognition","video_url":null},{"abstract":"Automatic discourse processing is bottlenecked by data: current discourse formalisms pose highly demanding annotation tasks involving large taxonomies of discourse relations, making them inaccessible to lay annotators. This work instead adopts the linguistic framework of Questions Under Discussion (QUD) for discourse analysis and seeks to derive QUD structures automatically. QUD views each sentence as an answer to a question triggered in prior context; thus, we characterize relationships between sentences as free-form questions, in contrast to exhaustive fine-grained taxonomies. We develop the first-of-its-kind QUD parser that derives a dependency structure of questions over full documents, trained using a large, crowdsourced question-answering dataset DCQA (Ko et al., 2022). Human evaluation results show that QUD dependency parsing is possible for language models trained with this crowdsourced, generalizable annotation scheme. We illustrate how our QUD structure is distinct from RST trees, and demonstrate the utility of QUD analysis in the context of document simplification. Our findings show that QUD parsing is an appealing alternative for automatic discourse processing.","anthology_url":"https://aclanthology.org/2023.findings-acl.710","authors":["Wei-Jen Ko","Yating Wu","Cutter J Dalton","Dananjay T Srinivas","Greg Durrett","Junyi Jessy Li"],"category":"Findings","demo_url":null,"display_track":"Discourse and Pragmatics","event_ids":["session-4_-discourse-and-pragmatics-(virtual-poster)","spotlight-session_-spotlight---metropolitan-west-(spotlight)"],"id":"P4813","is_paper":true,"keywords":["discourse parsing"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.710.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/78003/poster_document/f74f672eb92038309bf78758841f1d80.pdf","preview_image":"https://assets.underline.io/lecture/78003/poster/c240f43c089ff4fc8fd1cf7d90ad68d7.png","program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/78003/slideshow/2ba8f805a04774b784febedfbd43b113.pdf","title":"Discourse Analysis via Questions and Answers: Parsing Dependency Structures of Questions Under Discussion","tldr":"Automatic discourse processing is bottlenecked by data: current discourse formalisms pose highly demanding annotation tasks involving large taxonomies of discourse relations, making them inaccessible to lay annotators. This work instead adopts the linguistic framework of Questions Under Discussion (...","track":"Discourse and Pragmatics","underline_id":78003,"underline_url":"https://underline.io/events/395/posters/15240/poster/78003-discourse-analysis-via-questions-and-answers-parsing-dependency-structures-of-questions-under-discussion","video_url":null},{"abstract":"As the foundation of current natural language processing methods, pre-trained language model has achieved excellent performance. However, the black-box structure of the deep neural network in pre-trained language models seriously limits the interpretability of the language modeling process. After revisiting the coupled requirement of deep neural representation and semantics logic of language modeling, a Word-Context-Coupled Space (W2CSpace) is proposed by introducing the alignment processing between uninterpretable neural representation and interpretable statistical logic. Moreover, a clustering process is also designed to connect the word- and context-level semantics. Specifically, an associative knowledge network (AKN), considered interpretable statistical logic, is introduced in the alignment process for word-level semantics. Furthermore, the context-relative distance is employed as the semantic feature for the downstream classifier, which is greatly different from the current uninterpretable semantic representations of pre-trained models. Our experiments for performance evaluation and interpretable analysis are executed on several types of datasets, including SIGHAN, Weibo, and ChnSenti. Wherein a novel evaluation strategy for the interpretability of machine learning models is first proposed. According to the experimental results, our language model can achieve better performance and highly credible interpretable ability compared to related state-of-the-art methods.","anthology_url":"https://aclanthology.org/2023.findings-acl.532","authors":["Fanyu Wang","Zhenping Xie"],"category":"Findings","demo_url":null,"display_track":"Interpretability and Analysis of Models for NLP","event_ids":["session-1_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)"],"id":"P2562","is_paper":true,"keywords":["free-text/natural language explanations"],"languages":["chinese"],"material":null,"paper_pdf":"https://aclanthology.org/2023.findings-acl.532.pdf","paper_type":"findings","poster_pdf":"https://assets.underline.io/lecture/77667/poster_document/2978c19fe27499a6560f7c1bfb7b7fb1.pdf","preview_image":null,"program":"Findings","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/77667/slideshow/a4e5f1c10f60500403fe4582e96ec980.pdf","title":"Constructing Word-Context-Coupled Space Aligned with Associative Knowledge Relations for Interpretable Language Modeling","tldr":"As the foundation of current natural language processing methods, pre-trained language model has achieved excellent performance. However, the black-box structure of the deep neural network in pre-trained language models seriously limits the interpretability of the language modeling process. After re...","track":"Interpretability and Analysis of Models for NLP","underline_id":77667,"underline_url":"https://underline.io/events/395/posters/15200/poster/77667-constructing-word-context-coupled-space-aligned-with-associative-knowledge-relations-for-interpretable-language-modeling","video_url":null}]
