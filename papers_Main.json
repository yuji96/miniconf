[{"abstract":"Although proper handling of discourse significantly contributes to the quality of machine translation (MT), these improvements are not adequately measured in common translation quality metrics. Recent works in context-aware MT attempt to target a small set of discourse phenomena during evaluation, however not in a fully systematic way. In this paper, we develop the Multilingual Discourse-Aware (MuDA) benchmark, a series of taggers that identify and evaluate model performance on discourse phenomena in any given dataset. The choice of phenomena is inspired by a novel methodology to systematically identify translations that require context. This methodology confirms the difficulty of previously studied phenomena while uncovering others which were not previously addressed. We find that commonly studied context-aware MT models make only marginal improvements over context-agnostic models, which suggests these models do not handle these ambiguities effectively. We release code and data for 14 language pairs to encourage the MT community to focus on accurately capturing discourse phenomena. Code available at https://github.com/neulab/contextual-mt","anthology_url":"https://aclanthology.org/2023.acl-long.36","authors":["Patrick Fernandes","Kayo Yin","Emmy Liu","Andr\u00e9 Martins","Graham Neubig"],"category":"Main-Poster","demo_url":null,"display_track":"Machine Translation","event_ids":["poster-session-6_-machine-translation-(poster)"],"id":"P3784","is_paper":true,"keywords":["automatic evaluation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-long.36.pdf","paper_type":"Main-Poster","poster_pdf":"https://assets.underline.io/lecture/76921/poster_document/ccf344e4ccdce6d914b620edfd9bdb83.pdf","preview_image":"https://assets.underline.io/lecture/76921/poster/3f0b6b352305b21fa76ac1ada7a3e610.png","program":"Main","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/76921/slideshow/d2e02881e96b269d081c74a491b144e9.pdf","title":"When Does Translation Require Context? A Data-driven, Multilingual Exploration","tldr":"Although proper handling of discourse significantly contributes to the quality of machine translation (MT), these improvements are not adequately measured in common translation quality metrics. Recent works in context-aware MT attempt to target a small set of discourse phenomena during evaluation, h...","track":"Machine Translation","underline_id":76921,"underline_url":"https://underline.io/events/395/posters/15264/poster/76921-when-does-translation-require-contextquestion-a-data-driven-multilingual-exploration","video_url":null},{"abstract":"Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment classification task. Many recent works have used dependency trees to extract the relation between aspects and contexts and have achieved significant improvements. However, further improvement is limited due to the potential mismatch between the dependency tree as a syntactic structure and the sentiment classification as a semantic task. To alleviate this gap, we replace the syntactic dependency tree with the semantic structure named Abstract Meaning Representation (AMR) and propose a model called AMR-based Path Aggregation Relational Network (APARN) to take full advantage of semantic structures. In particular, we design the path aggregator and the relation-enhanced self-attention mechanism that complement each other. The path aggregator extracts semantic features from AMRs under the guidance of sentence information, while the relation-enhanced self-attention mechanism in turn improves sentence features with refined semantic information. Experimental results on four public datasets demonstrate 1.13\\% average F1 improvement of APARN in ABSA when compared with state-of-the-art baselines.","anthology_url":"https://aclanthology.org/2023.acl-long.19","authors":["Fukun Ma","Xuming Hu","Aiwei Liu","Yawen Yang","Shuang Li","Philip S. Yu","Lijie Wen"],"category":"Main-Poster","demo_url":null,"display_track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining","event_ids":["session-4_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)"],"id":"P1638","is_paper":true,"keywords":["stance detection"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-long.19.pdf","paper_type":"Main-Poster","poster_pdf":"https://assets.underline.io/lecture/76613/poster_document/9e796f0f6e5a5bd895907a53973b7314.pdf","preview_image":"https://assets.underline.io/lecture/76613/poster/81314e83ff0d161c085d6acfba700fc6.png","program":"Main","similar_paper_ids":[],"slides_pdf":null,"title":"AMR-based Network for Aspect-based Sentiment Analysis","tldr":"Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment classification task. Many recent works have used dependency trees to extract the relation between aspects and contexts and have achieved significant improvements. However, further improvement is limited due to the potential mismatch ...","track":"Sentiment Analysis, Stylistic Analysis, and Argument Mining","underline_id":76613,"underline_url":"https://underline.io/events/395/posters/15240/poster/76613-amr-based-network-for-aspect-based-sentiment-analysis","video_url":null},{"abstract":"Text style transfer (TST) is an important task in natural language generation, which aims to alter the stylistic attributes (e.g., sentiment) of a sentence and keep its semantic meaning unchanged. Most existing studies mainly focus on the transformation between styles, yet ignore that this transformation can be actually carried out via different hidden transfer patterns. To address this problem, we propose a novel approach, contrastive transfer pattern mining (CTPM), which automatically mines and utilizes inherent latent transfer patterns to improve the performance of TST. Specifically, we design an adaptive clustering module to automatically discover hidden transfer patterns from the data, and introduce contrastive learning based on the discovered patterns to obtain more accurate sentence representations, and thereby benefit the TST task. To the best of our knowledge, this is the first work that proposes the concept of transfer patterns in TST, and our approach can be applied in a plug-and-play manner to enhance other TST methods to further improve their performance. Extensive experiments on benchmark datasets verify the effectiveness and generality of our approach.","anthology_url":"https://aclanthology.org/2023.acl-long.439","authors":["Jingxuan Han","Quan Wang","Licheng Zhang","Weidong Chen","Yan Song","Zhendong Mao"],"category":"Main-Poster","demo_url":null,"display_track":"Generation","event_ids":["poster-session-1_-generation-(poster)"],"id":"P3117","is_paper":true,"keywords":["text-to-text generation"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-long.439.pdf","paper_type":"Main-Poster","poster_pdf":"https://assets.underline.io/lecture/76822/poster_document/760c270350fb6409caa40bc180f7b63d.pdf","preview_image":null,"program":"Main","similar_paper_ids":[],"slides_pdf":null,"title":"Text Style Transfer with Contrastive Transfer Pattern Mining","tldr":"Text style transfer (TST) is an important task in natural language generation, which aims to alter the stylistic attributes (e.g., sentiment) of a sentence and keep its semantic meaning unchanged. Most existing studies mainly focus on the transformation between styles, yet ignore that this transform...","track":"Generation","underline_id":76822,"underline_url":"https://underline.io/events/395/posters/15197/poster/76822-interpret-positional-information-in-perspective-of-word-order","video_url":null},{"abstract":"Although achieving promising performance, current Natural Language Understanding models tend to utilize dataset biases instead of learning the intended task, which always leads to performance degradation on out-of-distribution (OOD) samples. To\nincrease the performance stability, previous debiasing methods \\emph{empirically} capture bias features from data to prevent the model from corresponding biases. However, our analyses show that the empirical debiasing methods may fail to capture part of the potential dataset biases and mistake semantic information of input text as biases, which limits the effectiveness of debiasing. To address these issues, we propose a debiasing framework IEGDB that comprehensively detects the dataset biases to induce a set of biased features, and then purifies the biased features with the guidance of information entropy. Experimental results show that IEGDB can consistently improve the stability of performance on OOD datasets for a set of widely adopted NLU models.","anthology_url":"https://aclanthology.org/2023.acl-long.161","authors":["Li Du","Xiao Ding","Zhouhao Sun","Ting Liu","Bing Qin","Jingshuo Liu"],"category":"Main-Poster","demo_url":null,"display_track":"Interpretability and Analysis of Models for NLP","event_ids":["session-1_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)"],"id":"P5090","is_paper":true,"keywords":["robustness"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-long.161.pdf","paper_type":"Main-Poster","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/77105/poster/a8a0263eb84feab02e27accd3fff66a0.jpg","program":"Main","similar_paper_ids":[],"slides_pdf":null,"title":"Towards Stable Natural Language Understanding via Information Entropy Guided Debiasing","tldr":"Although achieving promising performance, current Natural Language Understanding models tend to utilize dataset biases instead of learning the intended task, which always leads to performance degradation on out-of-distribution (OOD) samples. To\nincrease the performance stability, previous debiasing ...","track":"Interpretability and Analysis of Models for NLP","underline_id":77105,"underline_url":"https://underline.io/events/395/posters/15200/poster/77105-towards-stable-natural-language-understanding-via-information-entropy-guided-debiasing","video_url":null},{"abstract":"In long document controllable summarization, where labeled data is scarce, pretrained models struggle to adapt to the task and effectively respond to user queries. In this paper, we introduce Socratic pretraining, a question-driven, unsupervised pretraining objective specifically designed to improve controllability in summarization tasks. By training a model to generate and answer relevant questions in a given context, Socratic pretraining enables the model to more effectively adhere to user-provided queries and identify relevant content to be summarized. We demonstrate the effectiveness of this approach through extensive experimentation on two summarization domains, short stories and dialogue, and multiple control strategies: keywords, questions, and factoid QA pairs. Our pretraining method relies only on unlabeled documents and a question generation system and outperforms pre-finetuning approaches that use additional supervised data. Furthermore, our results show that Socratic pretraining cuts task-specific labeled data requirements in half, is more faithful to user-provided queries, and achieves state-of-the-art performance on QMSum and SQuALITY.","anthology_url":"https://aclanthology.org/2023.acl-long.713","authors":["Artidoro Pagnoni","Alex Fabbri","Wojciech Kryscinski","Chien-Sheng Jason Wu"],"category":"Main-Oral","demo_url":null,"display_track":"Summarization","event_ids":["session-4_-summarization-(oral)"],"id":"P970","is_paper":true,"keywords":["query-focused summarization"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-long.713.pdf","paper_type":"Main-Oral","poster_pdf":null,"preview_image":"https://assets.underline.io/lecture/76214/poster/0bb5868fa791b1c638a6cdcf3bcccf1b.jpg","program":"Main","similar_paper_ids":[],"slides_pdf":null,"title":"Socratic Pretraining: Question-Driven Pretraining for Controllable Summarization","tldr":"In long document controllable summarization, where labeled data is scarce, pretrained models struggle to adapt to the task and effectively respond to user queries. In this paper, we introduce Socratic pretraining, a question-driven, unsupervised pretraining objective specifically designed to improve...","track":"Summarization","underline_id":76214,"underline_url":"https://underline.io/events/395/sessions/15235/lecture/76214-socratic-pretraining-question-driven-pretraining-for-controllable-summarization","video_url":null},{"abstract":"Verbal omissions are complex syntactic phenomena in VP coordination structures. They occur when verbs and (some of) their arguments are omitted from subsequent clauses after being explicitly stated in an initial clause. Recovering these omitted elements is necessary for accurate interpretation of the sentence, and while humans easily and intuitively fill in the missing information, state-of-the-art models continue to struggle with this task. Previous work is limited to small-scale datasets, synthetic data creation methods, and to resolution methods in the dependency-graph level. In this work we propose a {\\em conjunct resolution} task that operates directly on the text and makes use of a {\\em split-and-rephrase} paradigm in order to recover the missing elements in the coordination structure. To this end, we first formulate a pragmatic framework of verbal omissions which describes the different types of omissions, and develop an automatic scalable collection method. Based on this method, we curate a large dataset, containing over 10K examples of naturally-occurring verbal omissions with crowd-sourced annotations of the resolved conjuncts. We train various neural baselines for this task, and show that while our best method obtains decent performance, it leaves ample space for improvement. We propose our dataset, metrics and models as a starting point for future research on this topic.","anthology_url":"https://aclanthology.org/2023.acl-long.762","authors":["Royi Rassin","Yoav Goldberg","Reut Tsarfaty"],"category":"Main-Poster","demo_url":null,"display_track":"Semantics: Sentence-level Semantics, Textual Inference, and Other Areas","event_ids":["poster-session-7_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)"],"id":"P2152","is_paper":true,"keywords":["reasoning","paraphrase generation","text simplification"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-long.762.pdf","paper_type":"Main-Poster","poster_pdf":"https://assets.underline.io/lecture/76682/poster_document/57e985c4511471eb83ffce7e371d0cda.pdf","preview_image":"https://assets.underline.io/lecture/76682/poster/558a782d4740fec0eab6f1823b460366.png","program":"Main","similar_paper_ids":[],"slides_pdf":null,"title":"Conjunct Resolution in the Face of Verbal Omissions","tldr":"Verbal omissions are complex syntactic phenomena in VP coordination structures. They occur when verbs and (some of) their arguments are omitted from subsequent clauses after being explicitly stated in an initial clause. Recovering these omitted elements is necessary for accurate interpretation of th...","track":"Semantics: Sentence-level Semantics, Textual Inference, and Other Areas","underline_id":76682,"underline_url":"https://underline.io/events/395/posters/15298/poster/76682-cobra-frames-contextual-reasoning-about-effects-and-harms-of-offensive-statements","video_url":null},{"abstract":"One of the main challenges open-domain end-to-end dialogue systems, or chatbots, face is the prevalence of unsafe behavior, such as toxic languages and harmful suggestions. However, existing dialogue datasets do not provide enough annotation to explain and correct such unsafe behavior. In this work, we construct a new dataset called SafeConv for the research of conversational safety: (1) Besides the utterance-level safety labels, SafeConv also provides unsafe spans in an utterance, information able to indicate which words contribute to the detected unsafe behavior; (2) SafeConv provides safe alternative responses to continue the conversation when unsafe behavior detected, guiding the conversation to a gentle trajectory. \n\nBy virtue of the comprehensive annotation of SafeConv, we benchmark three powerful models for the mitigation of conversational unsafe behavior, including a checker to detect unsafe utterances, a tagger to extract unsafe spans, and a rewriter to convert an unsafe response to a safe version. Moreover, we explore the huge benefits brought by combining the models for explaining the emergence of unsafe behavior and detoxifying chatbots. Experiments show that the detected unsafe behavior could be well explained with unsafe spans and popular chatbots could be detoxified by a huge extent. The dataset is available at https://github.com/mianzhang/SafeConv.","anthology_url":"https://aclanthology.org/2023.acl-long.2","authors":["Mian Zhang","Lifeng Jin","Linfeng Song","Haitao Mi","Wenliang Chen","Dong Yu"],"category":"Main-Oral","demo_url":null,"display_track":"Resources and Evaluation","event_ids":["session-4_-resources-and-evaluation-(oral)"],"id":"P1626","is_paper":true,"keywords":["corpus creation","benchmarking","nlp datasets"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-long.2.pdf","paper_type":"Main-Oral","poster_pdf":"https://assets.underline.io/lecture/76227/poster_document/0299acb5c4aef0941dfe58944b78c8d4.pdf","preview_image":"https://assets.underline.io/lecture/76227/poster/d7e40e3009397a8bb9a91901c7be9c15.png","program":"Main","similar_paper_ids":[],"slides_pdf":"https://assets.underline.io/lecture/76227/slideshow/e6eb4b7100fb906457e8b958e217ffe7.pdf","title":"SafeConv: Explaining and Correcting Conversational Unsafe Behavior","tldr":"One of the main challenges open-domain end-to-end dialogue systems, or chatbots, face is the prevalence of unsafe behavior, such as toxic languages and harmful suggestions. However, existing dialogue datasets do not provide enough annotation to explain and correct such unsafe behavior. In this work,...","track":"Resources and Evaluation","underline_id":76227,"underline_url":"https://underline.io/events/395/sessions/15233/lecture/76227-safeconv-explaining-and-correcting-conversational-unsafe-behavior","video_url":null},{"abstract":"Remarkable progress has been made on automated reasoning with natural text, by using Large Language Models (LLMs) and methods such as Chain-of-Thought prompting and Selection-Inference. These techniques search for proofs in the forward direction from axioms to the conclusion, which suffers from a combinatorial explosion of the search space, and thus high failure rates for problems requiring longer chains of reasoning. The classical automated reasoning literature has shown that reasoning in the backward direction (i.e. from intended conclusion to supporting axioms) is significantly more efficient at proof-finding. Importing this intuition into the LM setting, we develop a Backward Chaining algorithm, called LAMBADA, that decomposes reasoning into four sub-modules, that are simply implemented by few-shot prompted LLM inference. We show that LAMBADA achieves sizable accuracy boosts over state-of-the-art forward reasoning methods on two challenging logical reasoning datasets, particularly when deep and accurate proof chains are required.","anthology_url":"https://aclanthology.org/2023.acl-long.361","authors":["Mehran Kazemi","Najoung Kim","Deepti Bhatia","Xin Xu","Deepak Ramachandran"],"category":"Main-Poster","demo_url":null,"display_track":"Large Language Models","event_ids":["poster-session-5_-large-language-models-(poster)"],"id":"P2012","is_paper":true,"keywords":["prompting"],"languages":[],"material":null,"paper_pdf":"https://aclanthology.org/2023.acl-long.361.pdf","paper_type":"Main-Poster","poster_pdf":"https://assets.underline.io/lecture/76658/poster_document/1d532f4c2fc851c73c2deabd6f8dffd2.pdf","preview_image":"https://assets.underline.io/lecture/76658/poster/864cbe5da4b5ca41aa33e0f80c8205fc.png","program":"Main","similar_paper_ids":[],"slides_pdf":null,"title":"LAMBADA: Backward Chaining for Automated Reasoning in Natural Language","tldr":"Remarkable progress has been made on automated reasoning with natural text, by using Large Language Models (LLMs) and methods such as Chain-of-Thought prompting and Selection-Inference. These techniques search for proofs in the forward direction from axioms to the conclusion, which suffers from a co...","track":"Large Language Models","underline_id":76658,"underline_url":"https://underline.io/events/395/posters/15254/poster/76658-lambada-backward-chaining-for-automated-reasoning-in-natural-language","video_url":null}]
