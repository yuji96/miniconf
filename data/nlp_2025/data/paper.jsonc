{
  "abstract": "TBA",
  // "anthology_url": "https://aclanthology.org/2023.findings-acl.625",
  "authors": [
    "新井 雅稀",
    "芝原 俊樹",
    "千葉 大紀",
    "秋山 満昭",
    "内田 真人"
  ],
  // "category": "Findings",
  // "demo_url": null,
  // "display_track": "Machine Learning for NLP",
  // "event_ids": [
  //   "session-1_-machine-learning-for-nlp-(virtual-poster)",
  //   "spotlight-session_-spotlight---metropolitan-centre-(spotlight)"
  // ],
  "id": "A1-1",
  // "is_paper": true,
  // "keywords": [
  //   "representation learning",
  //   "model compression methods"
  // ],
  // "languages": [],
  // "material": null,
  "paper_pdf": "TBA",
  // "paper_type": "findings",
  // "poster_pdf": "https://assets.underline.io/lecture/77434/poster_document/280fbeb68b32fcf5c1475fea7712157c.pdf",
  // "preview_image": "https://assets.underline.io/lecture/77434/poster/968641458a34e92b5096e7593aefb875.jpg",
  // "program": "Findings",
  // "similar_paper_ids": [],
  // "slides_pdf": "https://assets.underline.io/lecture/77434/slideshow/606333221cf40a7592eb718693d11af0.pdf",
  "title": "LLMのアテンションヘッドに着目したジェイルブレイク攻撃の分析と防御手法の提案",
  // "tldr": "The size of embeddings generated by large language models can negatively affect system latency and model size in certain downstream practical applications (e.g. KNN search). In this work, we propose EmbedTextNet, a light add-on network that can be appended to an arbitrary language model to generate ...",
  // "track": "Machine Learning for NLP",
  // "underline_id": 77434,
  // "underline_url": "https://underline.io/events/395/posters/15200/poster/77434-propsegment-a-large-scale-corpus-for-proposition-level-segmentation-and-entailment-recognition",
  // "video_url": null
}