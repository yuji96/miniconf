{
  "plenaries": {},
  "tutorials": {},
  "workshops": {},
  "sessions": {
    "AmericasNLP": {
      "display_name": "W20 - The 3rd Workshop on NLP for Indigenous Languages of the Americas (AmericasNLP)",
      "start_time": "2025-03-10T09:00:00+09:00",
      "end_time": "2025-03-10T17:00:00+09:00",
      "events": {},
      "id": "AmericasNLP",
      "name": "W20 - The 3rd Workshop on NLP for Indigenous Languages of the Americas (AmericasNLP)",
      "plenary_events": {},
      
      "tutorial_events": {},
      "type": "Workshops",
      "workshop_events": {
        "AmericasNLP": {
          "anthology_venue_id": "ACL",
          "booklet_id": "workshop_20",
          "chairs": [],
          "committee": [],
          "description": "AmericasNLP aims to (a) encourage research on NLP, computational linguistics, corpus linguistics, and speech around the globe to work on native American languages; (b) )connect researchers and professionals from underrepresented communities and native speakers of endangered languages with the machine learning and natural language processing communities; and (c) )promote research on both neural and non-neural machine learning approaches suitable for low-resource languages.",
          "end_time": null,
          "id": "AmericasNLP",
          "link": null,
          "paper_ids": [],
          "room": "Pier 3",
          "session": "AmericasNLP",
          "short_name": "AmericasNLP",
          "start_time": null,
          "track": "Workshop",
          "type": "Workshops",
          "workshop_site_url": "https://turing.iimas.unam.mx/americasnlp/"
        }
      }
    }
  },
  "events": {},
  "papers": [
    [
      "P1258",
      {
        "abstract": "The size of embeddings generated by large language models can negatively affect system latency and model size in certain downstream practical applications (e.g. KNN search). In this work, we propose EmbedTextNet, a light add-on network that can be appended to an arbitrary language model to generate a compact embedding without requiring any changes in its architecture or training procedure. Specifically, we use a correlation penalty added to the weighted reconstruction loss that better captures the informative features in the text embeddings, which improves the efficiency of the language models. We evaluated EmbedTextNet on three different downstream tasks: text similarity, language modelling, and text retrieval. Empirical results on diverse benchmark datasets demonstrate the effectiveness and superiority of EmbedTextNet compared to state-of-art methodologies in recent works, especially in extremely low dimensional embedding sizes. The developed code for reproducibility is included in the supplementary material.",
        "anthology_url": "https://aclanthology.org/2023.findings-acl.625",
        "authors": [
          "Dae Yon Hwang",
          "Bilal Taha",
          "Yaroslav Nechaev"
        ],
        "category": "Findings",
        "demo_url": null,
        "display_track": "Machine Learning for NLP",
        "event_ids": [
          "session-1_-machine-learning-for-nlp-(virtual-poster)",
          "spotlight-session_-spotlight---metropolitan-centre-(spotlight)"
        ],
        "id": "P1258",
        "is_paper": true,
        "keywords": [
          "representation learning",
          "model compression methods"
        ],
        "languages": [],
        "material": null,
        "paper_pdf": "https://aclanthology.org/2023.findings-acl.625.pdf",
        "paper_type": "findings",
        "poster_pdf": "https://assets.underline.io/lecture/77434/poster_document/280fbeb68b32fcf5c1475fea7712157c.pdf",
        "preview_image": "https://assets.underline.io/lecture/77434/poster/968641458a34e92b5096e7593aefb875.jpg",
        "program": "Findings",
        "similar_paper_ids": [],
        "slides_pdf": "https://assets.underline.io/lecture/77434/slideshow/606333221cf40a7592eb718693d11af0.pdf",
        "title": "EmbedTextNet: Dimension Reduction with Weighted Reconstruction and Correlation Losses for Efficient Text Embedding",
        "tldr": "The size of embeddings generated by large language models can negatively affect system latency and model size in certain downstream practical applications (e.g. KNN search). In this work, we propose EmbedTextNet, a light add-on network that can be appended to an arbitrary language model to generate ...",
        "track": "Machine Learning for NLP",
        "underline_id": 77434,
        "underline_url": "https://underline.io/events/395/posters/15200/poster/77434-propsegment-a-large-scale-corpus-for-proposition-level-segmentation-and-entailment-recognition",
        "video_url": null
      }
    ],
    [
      "P4813",
      {
        "abstract": "Automatic discourse processing is bottlenecked by data: current discourse formalisms pose highly demanding annotation tasks involving large taxonomies of discourse relations, making them inaccessible to lay annotators. This work instead adopts the linguistic framework of Questions Under Discussion (QUD) for discourse analysis and seeks to derive QUD structures automatically. QUD views each sentence as an answer to a question triggered in prior context; thus, we characterize relationships between sentences as free-form questions, in contrast to exhaustive fine-grained taxonomies. We develop the first-of-its-kind QUD parser that derives a dependency structure of questions over full documents, trained using a large, crowdsourced question-answering dataset DCQA (Ko et al., 2022). Human evaluation results show that QUD dependency parsing is possible for language models trained with this crowdsourced, generalizable annotation scheme. We illustrate how our QUD structure is distinct from RST trees, and demonstrate the utility of QUD analysis in the context of document simplification. Our findings show that QUD parsing is an appealing alternative for automatic discourse processing.",
        "anthology_url": "https://aclanthology.org/2023.findings-acl.710",
        "authors": [
          "Wei-Jen Ko",
          "Yating Wu",
          "Cutter J Dalton",
          "Dananjay T Srinivas",
          "Greg Durrett",
          "Junyi Jessy Li"
        ],
        "category": "Findings",
        "demo_url": null,
        "display_track": "Discourse and Pragmatics",
        "event_ids": [
          "session-4_-discourse-and-pragmatics-(virtual-poster)",
          "spotlight-session_-spotlight---metropolitan-west-(spotlight)"
        ],
        "id": "P4813",
        "is_paper": true,
        "keywords": [
          "discourse parsing"
        ],
        "languages": [],
        "material": null,
        "paper_pdf": "https://aclanthology.org/2023.findings-acl.710.pdf",
        "paper_type": "findings",
        "poster_pdf": "https://assets.underline.io/lecture/78003/poster_document/f74f672eb92038309bf78758841f1d80.pdf",
        "preview_image": "https://assets.underline.io/lecture/78003/poster/c240f43c089ff4fc8fd1cf7d90ad68d7.png",
        "program": "Findings",
        "similar_paper_ids": [],
        "slides_pdf": "https://assets.underline.io/lecture/78003/slideshow/2ba8f805a04774b784febedfbd43b113.pdf",
        "title": "Discourse Analysis via Questions and Answers: Parsing Dependency Structures of Questions Under Discussion",
        "tldr": "Automatic discourse processing is bottlenecked by data: current discourse formalisms pose highly demanding annotation tasks involving large taxonomies of discourse relations, making them inaccessible to lay annotators. This work instead adopts the linguistic framework of Questions Under Discussion (...",
        "track": "Discourse and Pragmatics",
        "underline_id": 78003,
        "underline_url": "https://underline.io/events/395/posters/15240/poster/78003-discourse-analysis-via-questions-and-answers-parsing-dependency-structures-of-questions-under-discussion",
        "video_url": null
      }
    ],
    [
      "ACL_5",
      {
        "abstract": "Sentence embeddings induced with various transformer architectures encode much semantic and syntactic information in a distributed manner in a one-dimensional array. We investigate whether specific grammatical information can be accessed in these distributed representations. Using data from a task developed to test rule-like generalizations, our experiments on detecting subject-verb agreement yield several promising results. First, we show that while the usual sentence representations encoded as one-dimensional arrays do not easily support extraction of rule-like regularities, a two-dimensional  reshaping of these vectors allows various learning architectures to access such information. Next, we show that various architectures can detect patterns in these two-dimensional reshaped sentence embeddings and successfully learn a model based on smaller amounts of simpler training data, which performs well on more complex test data. This indicates that current sentence embeddings contain information that is regularly distributed, and which can be captured when the embeddings are reshaped into higher dimensional arrays. Our results cast light on representations produced by language models and help move towards developing few-shot learning approaches.",
        "anthology_url": null,
        "authors": [
          "Vivi Nastase",
          "Paola Merlo"
        ],
        "category": "Workshop",
        "demo_url": null,
        "display_track": null,
        "event_ids": [
          "RepL4NLP"
        ],
        "id": "ACL_5",
        "is_paper": true,
        "keywords": [],
        "languages": [],
        "material": null,
        "paper_pdf": null,
        "paper_type": "N/A",
        "poster_pdf": null,
        "preview_image": null,
        "program": "Workshop",
        "similar_paper_ids": [],
        "slides_pdf": null,
        "title": "Grammatical information in BERT sentence embeddings as two-dimensional arrays",
        "tldr": "Sentence embeddings induced with various transformer architectures encode much semantic and syntactic information in a distributed manner in a one-dimensional array. We investigate whether specific grammatical information can be accessed in these distributed representations. Using data from a task d",
        "track": "The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)",
        "underline_id": null,
        "underline_url": null,
        "video_url": null
      }
    ],
    [
      "I3",
      {
        "abstract": "In this work, we report our efforts in advancing Chinese Word Segmentation for the purpose of rapid deployment in different applications. The pre-trained language model (PLM) based segmentation methods have achieved state-of-the-art (SOTA) performance, whereas this paradigm also poses challenges in the deployment. It includes the balance between performance and cost, segmentation ambiguity due to domain diversity and vague words boundary, and multi-grained segmentation. In this context, we propose a simple yet effective approach, namely CWSeg, to augment PLM-based schemes by developing cohort training and versatile decoding strategies. Extensive experiments on benchmark datasets demonstrate the efficiency and generalization of our approach. The corresponding segmentation system is also implemented for practical usage and the demo is recorded.",
        "anthology_url": "https://aclanthology.org/2023.acl-industry.1",
        "authors": [
          "Dedong Li",
          "Rui Zhao",
          "Fei Tan"
        ],
        "category": "Industry",
        "demo_url": null,
        "display_track": "Industry",
        "event_ids": [
          "session-5_-industry-(poster)"
        ],
        "id": "I3",
        "is_paper": true,
        "keywords": [],
        "languages": [],
        "material": null,
        "paper_pdf": "https://aclanthology.org/2023.acl-industry.1.pdf",
        "paper_type": "industry",
        "poster_pdf": null,
        "preview_image": null,
        "program": "Industry",
        "similar_paper_ids": [],
        "slides_pdf": null,
        "title": "[Industry] CWSeg: An Efficient and General Approach to Chinese Word Segmentation",
        "tldr": "In this work, we report our efforts in advancing Chinese Word Segmentation for the purpose of rapid deployment in different applications. The pre-trained language model (PLM) based segmentation methods have achieved state-of-the-art (SOTA) performance, whereas this paradigm also poses challenges in ...",
        "track": "Industry",
        "underline_id": 79794,
        "underline_url": "https://underline.io/events/395/posters/15254/poster/79794-cwseg-an-efficient-and-general-approach-to-chinese-word-segmentation",
        "video_url": null
      }
    ],
    [
      "WASSA_108",
      {
        "abstract": "In this paper, we highlight our approach for the \"WASSA 2023 Shared-Task 1: Empathy Detection and Emotion Classification\". By accurately identifying emotions from textual sources of data, deep learning models can be trained to understand and interpret human emotions more effectively. The classification of emotions facilitates the creation of more emotionally intelligent systems that can better understand and respond to human emotions. We compared multiple transformer-based models for multi-label classification. Ensembling and oversampling were used to improve the performance of the system. A threshold-based voting mechanism performed on three models (Longformer, BERT, BigBird) yields the highest overall macro F1-score of 0.6605.",
        "anthology_url": null,
        "authors": [
          "Aditya Paranjape",
          "Gaurav Kolhatkar",
          "Yash Patwardhan",
          "Omkar Gokhale",
          "Shweta Dharmadhikari"
        ],
        "category": "Workshop",
        "demo_url": null,
        "display_track": null,
        "event_ids": [
          "WASSA"
        ],
        "id": "WASSA_108",
        "is_paper": true,
        "keywords": [],
        "languages": [],
        "material": null,
        "paper_pdf": null,
        "paper_type": "long",
        "poster_pdf": null,
        "preview_image": null,
        "program": "Workshop",
        "similar_paper_ids": [],
        "slides_pdf": null,
        "title": "Converge at WASSA 2023 Empathy, Emotion and Personality Shared Task: A Transformer-based Approach for Multi-Label Emotion Classification",
        "tldr": "In this paper, we highlight our approach for the \"WASSA 2023 Shared-Task 1: Empathy Detection and Emotion Classification\". By accurately identifying emotions from textual sources of data, deep learning models can be trained to understand and interpret human emotions more effectively. The classificat",
        "track": "The 13th Workshop on Computational Approaches to Subjectivity, Sentiment, & Social Media Analysis",
        "underline_id": null,
        "underline_url": null,
        "video_url": null
      }
    ],
    [
      "D89",
      {
        "abstract": "There are a growing number of table pre-training methods proposed for reasoning over tabular data (e.g., question answering, fact checking, and faithful text generation). However, most existing methods are benchmarked solely on a limited number of datasets, varying in configuration, which leads to a lack of unified, standardized, fair, and comprehensive comparison between methods. This paper presents OpenRT, the first open-source framework for reasoning over tabular data, to reproduce existing table pre-training models for performance comparison and develop new models quickly. We implemented and compared six table pre-training models on four question answering, one fact checking, and one faithful text generation datasets. Moreover, to enable the community to easily construct new table reasoning datasets, we developed TaRAT, an annotation tool which supports multi-person collaborative annotations for various kinds of table reasoning tasks. The researchers are able to deploy the newly-constructed dataset to OpenRT and compare the performances of different baseline systems.",
        "anthology_url": "https://aclanthology.org/2023.acl-demo.32",
        "authors": [
          "Yilun Zhao",
          "Boyu Mi",
          "Zhenting Qi",
          "Linyong Nan",
          "Minghao Guo",
          "Arman Cohan",
          "Dragomir Radev"
        ],
        "category": "Demo",
        "demo_url": null,
        "display_track": "Information Extraction (demo)",
        "event_ids": [
          "demo-session-3_-information-extraction-(demo)-(poster)"
        ],
        "id": "D89",
        "is_paper": true,
        "keywords": [],
        "languages": [],
        "material": null,
        "paper_pdf": "https://aclanthology.org/2023.acl-demo.32.pdf",
        "paper_type": "demo",
        "poster_pdf": "https://assets.underline.io/lecture/78259/poster_document/5434aea21322285c28af9f6b133c5cf7.pdf",
        "preview_image": "https://assets.underline.io/lecture/78259/poster/255a27020e55344a879d6484255e220b.jpg",
        "program": "Demo",
        "similar_paper_ids": [],
        "slides_pdf": null,
        "title": "[Demo] OpenRT: An Open-source Framework for Reasoning Over Tabular Data",
        "tldr": "There are a growing number of table pre-training methods proposed for reasoning over tabular data (e.g., question answering, fact checking, and faithful text generation). However, most existing methods are benchmarked solely on a limited number of datasets, varying in configuration, which leads to a...",
        "track": "Information Extraction (demo)",
        "underline_id": 78259,
        "underline_url": "https://underline.io/events/395/posters/15228/poster/78259-cross-lingual-amr-aligner-paying-attention-to-cross-attention",
        "video_url": null
      }
    ],
    [
      "P2562",
      {
        "abstract": "As the foundation of current natural language processing methods, pre-trained language model has achieved excellent performance. However, the black-box structure of the deep neural network in pre-trained language models seriously limits the interpretability of the language modeling process. After revisiting the coupled requirement of deep neural representation and semantics logic of language modeling, a Word-Context-Coupled Space (W2CSpace) is proposed by introducing the alignment processing between uninterpretable neural representation and interpretable statistical logic. Moreover, a clustering process is also designed to connect the word- and context-level semantics. Specifically, an associative knowledge network (AKN), considered interpretable statistical logic, is introduced in the alignment process for word-level semantics. Furthermore, the context-relative distance is employed as the semantic feature for the downstream classifier, which is greatly different from the current uninterpretable semantic representations of pre-trained models. Our experiments for performance evaluation and interpretable analysis are executed on several types of datasets, including SIGHAN, Weibo, and ChnSenti. Wherein a novel evaluation strategy for the interpretability of machine learning models is first proposed. According to the experimental results, our language model can achieve better performance and highly credible interpretable ability compared to related state-of-the-art methods.",
        "anthology_url": "https://aclanthology.org/2023.findings-acl.532",
        "authors": [
          "Fanyu Wang",
          "Zhenping Xie"
        ],
        "category": "Findings",
        "demo_url": null,
        "display_track": "Interpretability and Analysis of Models for NLP",
        "event_ids": [
          "session-1_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)"
        ],
        "id": "P2562",
        "is_paper": true,
        "keywords": [
          "free-text/natural language explanations"
        ],
        "languages": [
          "chinese"
        ],
        "material": null,
        "paper_pdf": "https://aclanthology.org/2023.findings-acl.532.pdf",
        "paper_type": "findings",
        "poster_pdf": "https://assets.underline.io/lecture/77667/poster_document/2978c19fe27499a6560f7c1bfb7b7fb1.pdf",
        "preview_image": null,
        "program": "Findings",
        "similar_paper_ids": [],
        "slides_pdf": "https://assets.underline.io/lecture/77667/slideshow/a4e5f1c10f60500403fe4582e96ec980.pdf",
        "title": "Constructing Word-Context-Coupled Space Aligned with Associative Knowledge Relations for Interpretable Language Modeling",
        "tldr": "As the foundation of current natural language processing methods, pre-trained language model has achieved excellent performance. However, the black-box structure of the deep neural network in pre-trained language models seriously limits the interpretability of the language modeling process. After re...",
        "track": "Interpretability and Analysis of Models for NLP",
        "underline_id": 77667,
        "underline_url": "https://underline.io/events/395/posters/15200/poster/77667-constructing-word-context-coupled-space-aligned-with-associative-knowledge-relations-for-interpretable-language-modeling",
        "video_url": null
      }
    ],
    [
      "P3784",
      {
        "abstract": "Although proper handling of discourse significantly contributes to the quality of machine translation (MT), these improvements are not adequately measured in common translation quality metrics. Recent works in context-aware MT attempt to target a small set of discourse phenomena during evaluation, however not in a fully systematic way. In this paper, we develop the Multilingual Discourse-Aware (MuDA) benchmark, a series of taggers that identify and evaluate model performance on discourse phenomena in any given dataset. The choice of phenomena is inspired by a novel methodology to systematically identify translations that require context. This methodology confirms the difficulty of previously studied phenomena while uncovering others which were not previously addressed. We find that commonly studied context-aware MT models make only marginal improvements over context-agnostic models, which suggests these models do not handle these ambiguities effectively. We release code and data for 14 language pairs to encourage the MT community to focus on accurately capturing discourse phenomena. Code available at https://github.com/neulab/contextual-mt",
        "anthology_url": "https://aclanthology.org/2023.acl-long.36",
        "authors": [
          "Patrick Fernandes",
          "Kayo Yin",
          "Emmy Liu",
          "Andr\u00e9 Martins",
          "Graham Neubig"
        ],
        "category": "Main-Poster",
        "demo_url": null,
        "display_track": "Machine Translation",
        "event_ids": [
          "poster-session-6_-machine-translation-(poster)"
        ],
        "id": "P3784",
        "is_paper": true,
        "keywords": [
          "automatic evaluation"
        ],
        "languages": [],
        "material": null,
        "paper_pdf": "https://aclanthology.org/2023.acl-long.36.pdf",
        "paper_type": "Main-Poster",
        "poster_pdf": "https://assets.underline.io/lecture/76921/poster_document/ccf344e4ccdce6d914b620edfd9bdb83.pdf",
        "preview_image": "https://assets.underline.io/lecture/76921/poster/3f0b6b352305b21fa76ac1ada7a3e610.png",
        "program": "Main",
        "similar_paper_ids": [],
        "slides_pdf": "https://assets.underline.io/lecture/76921/slideshow/d2e02881e96b269d081c74a491b144e9.pdf",
        "title": "When Does Translation Require Context? A Data-driven, Multilingual Exploration",
        "tldr": "Although proper handling of discourse significantly contributes to the quality of machine translation (MT), these improvements are not adequately measured in common translation quality metrics. Recent works in context-aware MT attempt to target a small set of discourse phenomena during evaluation, h...",
        "track": "Machine Translation",
        "underline_id": 76921,
        "underline_url": "https://underline.io/events/395/posters/15264/poster/76921-when-does-translation-require-contextquestion-a-data-driven-multilingual-exploration",
        "video_url": null
      }
    ],
    [
      "CAWL_14",
      {
        "abstract": "Maltese is a low-resource language of Arabic and Romance origins written in Latin script. We explore the impact of transliterating Maltese into Arabic script on a number of downstream tasks. We compare multiple transliteration pipelines ranging from simple one-to-one character maps to more sophisticated alternatives that explore multiple possibilities or make use of manual linguistic annotations. We show that the sophisticated systems are consistently better than simpler systems, quantitatively and qualitatively. We also show transliterating Maltese can be considered as an option to improve the cross-lingual transfer capabilities.",
        "anthology_url": null,
        "authors": [
          "Kurt Micallef",
          "Fadhl Eryani",
          "Nizar Habash",
          "Houda Bouamor",
          "Claudia Borg"
        ],
        "category": "Workshop",
        "demo_url": null,
        "display_track": null,
        "event_ids": [
          "CAWL"
        ],
        "id": "CAWL_14",
        "is_paper": true,
        "keywords": [],
        "languages": [],
        "material": null,
        "paper_pdf": null,
        "paper_type": "",
        "poster_pdf": null,
        "preview_image": null,
        "program": "Workshop",
        "similar_paper_ids": [],
        "slides_pdf": null,
        "title": "Exploring the Impact of Transliteration on NLP Performance for Low-Resource Languages: The Case of Maltese and Arabic",
        "tldr": "Maltese is a low-resource language of Arabic and Romance origins written in Latin script. We explore the impact of transliterating Maltese into Arabic script on a number of downstream tasks. We compare multiple transliteration pipelines ranging from simple one-to-one character maps to more sophistic",
        "track": "The Workshop on Computation and Written Language (CAWL)",
        "underline_id": null,
        "underline_url": null,
        "video_url": null
      }
    ],
    [
      "ClinicalNLP_12",
      {
        "abstract": "Pretrained language models leverage self-supervised learning to use large amounts of unlabeled text for learning contextual representations of sequences. However, in the domain of medical conversations, the availability of large, public datasets is limited due to issues of privacy and data management. In this paper, we study the effectiveness of dialog-aware pretraining objectives and multiphase training in using unlabeled data to improve LMs training for medical utterance classification. The objectives of pretraining for dialog awareness involve tasks that take into account the structure of conversations, including features such as turn-taking and the roles of speakers. The multiphase training process uses unannotated data in a sequence that prioritizes similarities and connections between different domains. We empirically evaluate these methods on conversational dialog classification tasks in the medical and counseling domains, and find that multiphase training can help achieve higher performance than standard pretraining or finetuning.",
        "anthology_url": null,
        "authors": [
          "Do June Min",
          "Veronica Perez-Rosas",
          "Rada Mihalcea"
        ],
        "category": "Workshop",
        "demo_url": null,
        "display_track": null,
        "event_ids": [
          "Clinical-NLP"
        ],
        "id": "ClinicalNLP_12",
        "is_paper": true,
        "keywords": [],
        "languages": [],
        "material": null,
        "paper_pdf": null,
        "paper_type": "N/A",
        "poster_pdf": null,
        "preview_image": null,
        "program": "Workshop",
        "similar_paper_ids": [],
        "slides_pdf": null,
        "title": "Navigating Data Scarcity: Pretraining for Medical Utterance Classification",
        "tldr": "Pretrained language models leverage self-supervised learning to use large amounts of unlabeled text for learning contextual representations of sequences. However, in the domain of medical conversations, the availability of large, public datasets is limited due to issues of privacy and data managemen",
        "track": "The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)",
        "underline_id": null,
        "underline_url": null,
        "video_url": null
      }
    ],
    [
      "P1638",
      {
        "abstract": "Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment classification task. Many recent works have used dependency trees to extract the relation between aspects and contexts and have achieved significant improvements. However, further improvement is limited due to the potential mismatch between the dependency tree as a syntactic structure and the sentiment classification as a semantic task. To alleviate this gap, we replace the syntactic dependency tree with the semantic structure named Abstract Meaning Representation (AMR) and propose a model called AMR-based Path Aggregation Relational Network (APARN) to take full advantage of semantic structures. In particular, we design the path aggregator and the relation-enhanced self-attention mechanism that complement each other. The path aggregator extracts semantic features from AMRs under the guidance of sentence information, while the relation-enhanced self-attention mechanism in turn improves sentence features with refined semantic information. Experimental results on four public datasets demonstrate 1.13\\% average F1 improvement of APARN in ABSA when compared with state-of-the-art baselines.",
        "anthology_url": "https://aclanthology.org/2023.acl-long.19",
        "authors": [
          "Fukun Ma",
          "Xuming Hu",
          "Aiwei Liu",
          "Yawen Yang",
          "Shuang Li",
          "Philip S. Yu",
          "Lijie Wen"
        ],
        "category": "Main-Poster",
        "demo_url": null,
        "display_track": "Sentiment Analysis, Stylistic Analysis, and Argument Mining",
        "event_ids": [
          "session-4_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)"
        ],
        "id": "P1638",
        "is_paper": true,
        "keywords": [
          "stance detection"
        ],
        "languages": [],
        "material": null,
        "paper_pdf": "https://aclanthology.org/2023.acl-long.19.pdf",
        "paper_type": "Main-Poster",
        "poster_pdf": "https://assets.underline.io/lecture/76613/poster_document/9e796f0f6e5a5bd895907a53973b7314.pdf",
        "preview_image": "https://assets.underline.io/lecture/76613/poster/81314e83ff0d161c085d6acfba700fc6.png",
        "program": "Main",
        "similar_paper_ids": [],
        "slides_pdf": null,
        "title": "AMR-based Network for Aspect-based Sentiment Analysis",
        "tldr": "Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment classification task. Many recent works have used dependency trees to extract the relation between aspects and contexts and have achieved significant improvements. However, further improvement is limited due to the potential mismatch ...",
        "track": "Sentiment Analysis, Stylistic Analysis, and Argument Mining",
        "underline_id": 76613,
        "underline_url": "https://underline.io/events/395/posters/15240/poster/76613-amr-based-network-for-aspect-based-sentiment-analysis",
        "video_url": null
      }
    ],
    [
      "LAW_F7",
      {
        "abstract": "Text Style Transfer (TST) evaluation is, in practice, inconsistent. Therefore, we conduct a meta-analysis on human and automated TST evaluation and experimentation that thoroughly examines existing literature in the field. The meta-analysis reveals a substantial standardization gap in human and automated evaluation. In addition, we also find a validation gap: only few automated metrics have been validated using human experiments. To this end, we thoroughly scrutinize both the standardization and validation gap and reveal the resulting pitfalls. This work also paves the way to close the standardization and validation gap in TST evaluation by calling out requirements to be met by future research.",
        "anthology_url": null,
        "authors": [
          "Phil Ostheimer",
          "Mayank Kumar Nagda",
          "Marius Kloft",
          "Sophie Fellenz"
        ],
        "category": "Workshop",
        "demo_url": null,
        "display_track": null,
        "event_ids": [
          "LAW"
        ],
        "id": "LAW_F7",
        "is_paper": true,
        "keywords": [],
        "languages": [],
        "material": null,
        "paper_pdf": null,
        "paper_type": "",
        "poster_pdf": null,
        "preview_image": null,
        "program": "Workshop",
        "similar_paper_ids": [],
        "slides_pdf": null,
        "title": "A Call for Standardization and Validation of Text Style Transfer Evaluation",
        "tldr": "Text Style Transfer (TST) evaluation is, in practice, inconsistent. Therefore, we conduct a meta-analysis on human and automated TST evaluation and experimentation that thoroughly examines existing literature in the field. The meta-analysis reveals a substantial standardization gap in human and auto",
        "track": "The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\ @ ACL 2023",
        "underline_id": null,
        "underline_url": null,
        "video_url": null
      }
    ],
    [
      "SemEval_205",
      {
        "abstract": "This paper describes the system and experimental results of an ensemble-based approach tomultilingual framing detection for the submission of the ACCEPT team to the SemEval-2023 Task 3 on Framing Detection (Subtask 2). The approach is based on an ensemble that combines three different methods: a classifier based on large language models, a classifier based on static word embeddings, and an approach that uses external commonsense knowledge graphs, in particular, ConceptNet. The results of the three classification heads are aggregated into an overall prediction for each frame class.Our best submission yielded a micro F1-score of 50.69\\% (rank 10) and a macro F1-score of 50.20\\% (rank 3) for English articles. Our experimental results show that static word embeddings and knowledge graphs are useful components for frame detection, while the ensemble of all three methods combines the strengths of our three proposed methods. Through system ablations, we show that the commonsenseguided knowledge graphs are the outperforming method for many languages.",
        "anthology_url": null,
        "authors": [
          "Philipp Heinisch",
          "Moritz Plenz",
          "Anette Frank",
          "Philipp Cimiano"
        ],
        "category": "Workshop",
        "demo_url": null,
        "display_track": null,
        "event_ids": [
          "SemEval"
        ],
        "id": "SemEval_205",
        "is_paper": true,
        "keywords": [],
        "languages": [],
        "material": null,
        "paper_pdf": null,
        "paper_type": "Task 3: Detecting the Category, the Framing, and the Persuasion Techniques in Online News in a Multi-lingual Setup",
        "poster_pdf": null,
        "preview_image": null,
        "program": "Workshop",
        "similar_paper_ids": [],
        "slides_pdf": null,
        "title": "ACCEPT at SemEval-2023 Task 3: An Ensemble-based Approach to Multilingual Framing Detection",
        "tldr": "This paper describes the system and experimental results of an ensemble-based approach tomultilingual framing detection for the submission of the ACCEPT team to the SemEval-2023 Task 3 on Framing Detection (Subtask 2). The approach is based on an ensemble that combines three different methods: a cla",
        "track": "The 17th International Workshop on Semantic Evaluation (SemEval-2023)",
        "underline_id": null,
        "underline_url": null,
        "video_url": null
      }
    ],
    [
      "P3117",
      {
        "abstract": "Text style transfer (TST) is an important task in natural language generation, which aims to alter the stylistic attributes (e.g., sentiment) of a sentence and keep its semantic meaning unchanged. Most existing studies mainly focus on the transformation between styles, yet ignore that this transformation can be actually carried out via different hidden transfer patterns. To address this problem, we propose a novel approach, contrastive transfer pattern mining (CTPM), which automatically mines and utilizes inherent latent transfer patterns to improve the performance of TST. Specifically, we design an adaptive clustering module to automatically discover hidden transfer patterns from the data, and introduce contrastive learning based on the discovered patterns to obtain more accurate sentence representations, and thereby benefit the TST task. To the best of our knowledge, this is the first work that proposes the concept of transfer patterns in TST, and our approach can be applied in a plug-and-play manner to enhance other TST methods to further improve their performance. Extensive experiments on benchmark datasets verify the effectiveness and generality of our approach.",
        "anthology_url": "https://aclanthology.org/2023.acl-long.439",
        "authors": [
          "Jingxuan Han",
          "Quan Wang",
          "Licheng Zhang",
          "Weidong Chen",
          "Yan Song",
          "Zhendong Mao"
        ],
        "category": "Main-Poster",
        "demo_url": null,
        "display_track": "Generation",
        "event_ids": [
          "poster-session-1_-generation-(poster)"
        ],
        "id": "P3117",
        "is_paper": true,
        "keywords": [
          "text-to-text generation"
        ],
        "languages": [],
        "material": null,
        "paper_pdf": "https://aclanthology.org/2023.acl-long.439.pdf",
        "paper_type": "Main-Poster",
        "poster_pdf": "https://assets.underline.io/lecture/76822/poster_document/760c270350fb6409caa40bc180f7b63d.pdf",
        "preview_image": null,
        "program": "Main",
        "similar_paper_ids": [],
        "slides_pdf": null,
        "title": "Text Style Transfer with Contrastive Transfer Pattern Mining",
        "tldr": "Text style transfer (TST) is an important task in natural language generation, which aims to alter the stylistic attributes (e.g., sentiment) of a sentence and keep its semantic meaning unchanged. Most existing studies mainly focus on the transformation between styles, yet ignore that this transform...",
        "track": "Generation",
        "underline_id": 76822,
        "underline_url": "https://underline.io/events/395/posters/15197/poster/76822-interpret-positional-information-in-perspective-of-word-order",
        "video_url": null
      }
    ],
    [
      "IWSLT_47",
      {
        "abstract": "In this paper, we present the KU x Upstage team's submission for the Special Task on Formality Control on Spoken Language Translation, which involves translating English into four languages with diverse grammatical formality markers.  Our methodology comprises two primary components: 1) a language-specific data-driven approach, and 2) the generation of synthetic data through the employment of large-scale language models and empirically-grounded prompt engineering. By adapting methodologies and models to accommodate the unique linguistic properties of each language, we observe a notable enhancement in performance relative to the baseline, substantiating the heightened efficacy of data-driven approaches. Moreover, our devised prompt engineering strategy yields superior synthetic translation instances.",
        "anthology_url": null,
        "authors": [
          "Seugnjun Lee",
          "Hyeonseok Moon",
          "Chanjun Park",
          "Heuiseok Lim"
        ],
        "category": "Workshop",
        "demo_url": null,
        "display_track": null,
        "event_ids": [
          "IWSLT"
        ],
        "id": "IWSLT_47",
        "is_paper": true,
        "keywords": [],
        "languages": [],
        "material": null,
        "paper_pdf": null,
        "paper_type": "long",
        "poster_pdf": null,
        "preview_image": null,
        "program": "Workshop",
        "similar_paper_ids": [],
        "slides_pdf": null,
        "title": "Improving Formality-Sensitive Machine Translation Using Data-Centric Approaches and Prompt Engineering",
        "tldr": "In this paper, we present the KU x Upstage team's submission for the Special Task on Formality Control on Spoken Language Translation, which involves translating English into four languages with diverse grammatical formality markers.  Our methodology comprises two primary components: 1) a language-s",
        "track": "The 20th International Conference on Spoken Language Translation",
        "underline_id": null,
        "underline_url": null,
        "video_url": null
      }
    ],
    [
      "SemEval_120",
      {
        "abstract": "This study describes the model design of the NCUEE-NLP system for the SemEval-2023 NLI4CT task that focuses on multi-evidence natural language inference for clinical trial data. We use the LinkBERT transformer in the biomedical domain (denoted as BioLinkBERT) as our main system architecture. First, a set of sentences in clinical trial reports is extracted as evidence for premise-statement inference. This identified evidence is then used to determine the inference relation (i.e., entailment or contradiction). Finally, a soft voting ensemble mechanism is applied to enhance the system performance. For Subtask 1 on textual entailment, our best submission had an F1-score of 0.7091, ranking sixth among all 30 participating teams. For Subtask 2 on evidence retrieval, our best result obtained an F1-score of 0.7940, ranking ninth of 19 submissions.",
        "anthology_url": null,
        "authors": [
          "Chao-Yi Chen",
          "Kao-Yuan Tien",
          "Yuan-Hao Cheng",
          "Lung-Hao Lee"
        ],
        "category": "Workshop",
        "demo_url": null,
        "display_track": null,
        "event_ids": [
          "SemEval"
        ],
        "id": "SemEval_120",
        "is_paper": true,
        "keywords": [],
        "languages": [],
        "material": null,
        "paper_pdf": null,
        "paper_type": "Task 7: Multi-Evidence Natural Language Inference for Clinical Trial Data",
        "poster_pdf": null,
        "preview_image": null,
        "program": "Workshop",
        "similar_paper_ids": [],
        "slides_pdf": null,
        "title": "NCUEE-NLP at SemEval-2023 Task 7: Ensemble Biomedical LinkBERT Transformers in Multi-evidence Natural Language Inference for Clinical Trial Data",
        "tldr": "This study describes the model design of the NCUEE-NLP system for the SemEval-2023 NLI4CT task that focuses on multi-evidence natural language inference for clinical trial data. We use the LinkBERT transformer in the biomedical domain (denoted as BioLinkBERT) as our main system architecture. First, ",
        "track": "The 17th International Workshop on Semantic Evaluation (SemEval-2023)",
        "underline_id": null,
        "underline_url": null,
        "video_url": null
      }
    ],
    [
      "SemEval_330",
      {
        "abstract": "This paper describes SinaAI's participation in SemEval-2023 Task 3, which involves detecting propaganda in news articles across multiple languages. The task comprises three sub-tasks: (i) genre detection, (ii) news framing,and (iii) persuasion technique identification.The employed dataset includes news articles in nine languages and domains, including English, French, Italian, German, Polish, Russian, Georgian, Greek, and Spanish, with labeled instances of news framing, genre, and persuasion techniques. Our approach combines fine-tuning  multilingual language models such as XLM, LaBSE, and mBERT with data augmentation techniques. Our experimental results show that XLM outperforms other models in terms of F1-Micro in and F1-Macro, and the ensemble of XLM and LaBSE achieved the best performance. Our study highlights the effectiveness of multilingual sentence embedding models in multilingual propaganda detection. Our models achieved highest score for two languages (greek and italy) in sub-task 1 and one language (Russian) for sub-task 2.",
        "anthology_url": null,
        "authors": [
          "Aryan Sadeghi",
          "Reza Alipour",
          "Kamyar Taeb",
          "Parimehr Morassafar",
          "Nima Salemahim",
          "Ehsaneddin Asgari"
        ],
        "category": "Workshop",
        "demo_url": null,
        "display_track": null,
        "event_ids": [
          "SemEval"
        ],
        "id": "SemEval_330",
        "is_paper": true,
        "keywords": [],
        "languages": [],
        "material": null,
        "paper_pdf": null,
        "paper_type": "Task 3: Detecting the Category, the Framing, and the Persuasion Techniques in Online News in a Multi-lingual Setup",
        "poster_pdf": null,
        "preview_image": null,
        "program": "Workshop",
        "similar_paper_ids": [],
        "slides_pdf": null,
        "title": "SinaAI at SemEval-2023 Task 3: A Multilingual Transformer Language Model-based Approach for the Detection of News Genre, Framing and Persuasion Techniques",
        "tldr": "This paper describes SinaAI's participation in SemEval-2023 Task 3, which involves detecting propaganda in news articles across multiple languages. The task comprises three sub-tasks: (i) genre detection, (ii) news framing,and (iii) persuasion technique identification.The employed dataset includes n",
        "track": "The 17th International Workshop on Semantic Evaluation (SemEval-2023)",
        "underline_id": null,
        "underline_url": null,
        "video_url": null
      }
    ],
    [
      "P5090",
      {
        "abstract": "Although achieving promising performance, current Natural Language Understanding models tend to utilize dataset biases instead of learning the intended task, which always leads to performance degradation on out-of-distribution (OOD) samples. To\nincrease the performance stability, previous debiasing methods \\emph{empirically} capture bias features from data to prevent the model from corresponding biases. However, our analyses show that the empirical debiasing methods may fail to capture part of the potential dataset biases and mistake semantic information of input text as biases, which limits the effectiveness of debiasing. To address these issues, we propose a debiasing framework IEGDB that comprehensively detects the dataset biases to induce a set of biased features, and then purifies the biased features with the guidance of information entropy. Experimental results show that IEGDB can consistently improve the stability of performance on OOD datasets for a set of widely adopted NLU models.",
        "anthology_url": "https://aclanthology.org/2023.acl-long.161",
        "authors": [
          "Li Du",
          "Xiao Ding",
          "Zhouhao Sun",
          "Ting Liu",
          "Bing Qin",
          "Jingshuo Liu"
        ],
        "category": "Main-Poster",
        "demo_url": null,
        "display_track": "Interpretability and Analysis of Models for NLP",
        "event_ids": [
          "session-1_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)"
        ],
        "id": "P5090",
        "is_paper": true,
        "keywords": [
          "robustness"
        ],
        "languages": [],
        "material": null,
        "paper_pdf": "https://aclanthology.org/2023.acl-long.161.pdf",
        "paper_type": "Main-Poster",
        "poster_pdf": null,
        "preview_image": "https://assets.underline.io/lecture/77105/poster/a8a0263eb84feab02e27accd3fff66a0.jpg",
        "program": "Main",
        "similar_paper_ids": [],
        "slides_pdf": null,
        "title": "Towards Stable Natural Language Understanding via Information Entropy Guided Debiasing",
        "tldr": "Although achieving promising performance, current Natural Language Understanding models tend to utilize dataset biases instead of learning the intended task, which always leads to performance degradation on out-of-distribution (OOD) samples. To\nincrease the performance stability, previous debiasing ...",
        "track": "Interpretability and Analysis of Models for NLP",
        "underline_id": 77105,
        "underline_url": "https://underline.io/events/395/posters/15200/poster/77105-towards-stable-natural-language-understanding-via-information-entropy-guided-debiasing",
        "video_url": null
      }
    ],
    [
      "P970",
      {
        "abstract": "In long document controllable summarization, where labeled data is scarce, pretrained models struggle to adapt to the task and effectively respond to user queries. In this paper, we introduce Socratic pretraining, a question-driven, unsupervised pretraining objective specifically designed to improve controllability in summarization tasks. By training a model to generate and answer relevant questions in a given context, Socratic pretraining enables the model to more effectively adhere to user-provided queries and identify relevant content to be summarized. We demonstrate the effectiveness of this approach through extensive experimentation on two summarization domains, short stories and dialogue, and multiple control strategies: keywords, questions, and factoid QA pairs. Our pretraining method relies only on unlabeled documents and a question generation system and outperforms pre-finetuning approaches that use additional supervised data. Furthermore, our results show that Socratic pretraining cuts task-specific labeled data requirements in half, is more faithful to user-provided queries, and achieves state-of-the-art performance on QMSum and SQuALITY.",
        "anthology_url": "https://aclanthology.org/2023.acl-long.713",
        "authors": [
          "Artidoro Pagnoni",
          "Alex Fabbri",
          "Wojciech Kryscinski",
          "Chien-Sheng Jason Wu"
        ],
        "category": "Main-Oral",
        "demo_url": null,
        "display_track": "Summarization",
        "event_ids": [
          "session-4_-summarization-(oral)"
        ],
        "id": "P970",
        "is_paper": true,
        "keywords": [
          "query-focused summarization"
        ],
        "languages": [],
        "material": null,
        "paper_pdf": "https://aclanthology.org/2023.acl-long.713.pdf",
        "paper_type": "Main-Oral",
        "poster_pdf": null,
        "preview_image": "https://assets.underline.io/lecture/76214/poster/0bb5868fa791b1c638a6cdcf3bcccf1b.jpg",
        "program": "Main",
        "similar_paper_ids": [],
        "slides_pdf": null,
        "title": "Socratic Pretraining: Question-Driven Pretraining for Controllable Summarization",
        "tldr": "In long document controllable summarization, where labeled data is scarce, pretrained models struggle to adapt to the task and effectively respond to user queries. In this paper, we introduce Socratic pretraining, a question-driven, unsupervised pretraining objective specifically designed to improve...",
        "track": "Summarization",
        "underline_id": 76214,
        "underline_url": "https://underline.io/events/395/sessions/15235/lecture/76214-socratic-pretraining-question-driven-pretraining-for-controllable-summarization",
        "video_url": null
      }
    ],
    [
      "IWSLT_4",
      {
        "abstract": "End-to-end automatic speech translation (AST) relies on data that combines audio inputs with text translation outputs. Previous work used existing large parallel corpora of transcriptions and translations in a knowledge distillation (KD) setup to distill a neural machine translation (NMT) into an AST student model. While KD allows using larger pretrained models, the reliance of previous KD approaches on manual audio transcripts in the data pipeline restricts the applicability of this framework to AST. We present an imitation learning approach where a teacher NMT system corrects the errors of an AST student without relying on manual transcripts. We show that the NMT teacher can recover from errors in automatic transcriptions and is able to correct erroneous translations of the AST student, leading to improvements of about 4 BLEU points over the standard AST end-to-end baseline on the English-German CoVoST-2 and MuST-C datasets, respectively. Code and data are publicly available: https://github.com/HubReb/imitkd_ast/releases/tag/v1.1",
        "anthology_url": null,
        "authors": [
          "Rebekka Hubert",
          "Artem Sokolov",
          "Stefan Riezler"
        ],
        "category": "Workshop",
        "demo_url": null,
        "display_track": null,
        "event_ids": [
          "IWSLT"
        ],
        "id": "IWSLT_4",
        "is_paper": true,
        "keywords": [],
        "languages": [],
        "material": null,
        "paper_pdf": null,
        "paper_type": "long",
        "poster_pdf": null,
        "preview_image": null,
        "program": "Workshop",
        "similar_paper_ids": [],
        "slides_pdf": null,
        "title": "Improving End-to-End Speech Translation by Imitation-Based Knowledge Distillation with Synthetic Transcripts",
        "tldr": "End-to-end automatic speech translation (AST) relies on data that combines audio inputs with text translation outputs. Previous work used existing large parallel corpora of transcriptions and translations in a knowledge distillation (KD) setup to distill a neural machine translation (NMT) into an AS",
        "track": "The 20th International Conference on Spoken Language Translation",
        "underline_id": null,
        "underline_url": null,
        "video_url": null
      }
    ],
    [
      "P2152",
      {
        "abstract": "Verbal omissions are complex syntactic phenomena in VP coordination structures. They occur when verbs and (some of) their arguments are omitted from subsequent clauses after being explicitly stated in an initial clause. Recovering these omitted elements is necessary for accurate interpretation of the sentence, and while humans easily and intuitively fill in the missing information, state-of-the-art models continue to struggle with this task. Previous work is limited to small-scale datasets, synthetic data creation methods, and to resolution methods in the dependency-graph level. In this work we propose a {\\em conjunct resolution} task that operates directly on the text and makes use of a {\\em split-and-rephrase} paradigm in order to recover the missing elements in the coordination structure. To this end, we first formulate a pragmatic framework of verbal omissions which describes the different types of omissions, and develop an automatic scalable collection method. Based on this method, we curate a large dataset, containing over 10K examples of naturally-occurring verbal omissions with crowd-sourced annotations of the resolved conjuncts. We train various neural baselines for this task, and show that while our best method obtains decent performance, it leaves ample space for improvement. We propose our dataset, metrics and models as a starting point for future research on this topic.",
        "anthology_url": "https://aclanthology.org/2023.acl-long.762",
        "authors": [
          "Royi Rassin",
          "Yoav Goldberg",
          "Reut Tsarfaty"
        ],
        "category": "Main-Poster",
        "demo_url": null,
        "display_track": "Semantics: Sentence-level Semantics, Textual Inference, and Other Areas",
        "event_ids": [
          "poster-session-7_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)"
        ],
        "id": "P2152",
        "is_paper": true,
        "keywords": [
          "reasoning",
          "paraphrase generation",
          "text simplification"
        ],
        "languages": [],
        "material": null,
        "paper_pdf": "https://aclanthology.org/2023.acl-long.762.pdf",
        "paper_type": "Main-Poster",
        "poster_pdf": "https://assets.underline.io/lecture/76682/poster_document/57e985c4511471eb83ffce7e371d0cda.pdf",
        "preview_image": "https://assets.underline.io/lecture/76682/poster/558a782d4740fec0eab6f1823b460366.png",
        "program": "Main",
        "similar_paper_ids": [],
        "slides_pdf": null,
        "title": "Conjunct Resolution in the Face of Verbal Omissions",
        "tldr": "Verbal omissions are complex syntactic phenomena in VP coordination structures. They occur when verbs and (some of) their arguments are omitted from subsequent clauses after being explicitly stated in an initial clause. Recovering these omitted elements is necessary for accurate interpretation of th...",
        "track": "Semantics: Sentence-level Semantics, Textual Inference, and Other Areas",
        "underline_id": 76682,
        "underline_url": "https://underline.io/events/395/posters/15298/poster/76682-cobra-frames-contextual-reasoning-about-effects-and-harms-of-offensive-statements",
        "video_url": null
      }
    ],
    [
      "P1626",
      {
        "abstract": "One of the main challenges open-domain end-to-end dialogue systems, or chatbots, face is the prevalence of unsafe behavior, such as toxic languages and harmful suggestions. However, existing dialogue datasets do not provide enough annotation to explain and correct such unsafe behavior. In this work, we construct a new dataset called SafeConv for the research of conversational safety: (1) Besides the utterance-level safety labels, SafeConv also provides unsafe spans in an utterance, information able to indicate which words contribute to the detected unsafe behavior; (2) SafeConv provides safe alternative responses to continue the conversation when unsafe behavior detected, guiding the conversation to a gentle trajectory. \n\nBy virtue of the comprehensive annotation of SafeConv, we benchmark three powerful models for the mitigation of conversational unsafe behavior, including a checker to detect unsafe utterances, a tagger to extract unsafe spans, and a rewriter to convert an unsafe response to a safe version. Moreover, we explore the huge benefits brought by combining the models for explaining the emergence of unsafe behavior and detoxifying chatbots. Experiments show that the detected unsafe behavior could be well explained with unsafe spans and popular chatbots could be detoxified by a huge extent. The dataset is available at https://github.com/mianzhang/SafeConv.",
        "anthology_url": "https://aclanthology.org/2023.acl-long.2",
        "authors": [
          "Mian Zhang",
          "Lifeng Jin",
          "Linfeng Song",
          "Haitao Mi",
          "Wenliang Chen",
          "Dong Yu"
        ],
        "category": "Main-Oral",
        "demo_url": null,
        "display_track": "Resources and Evaluation",
        "event_ids": [
          "session-4_-resources-and-evaluation-(oral)"
        ],
        "id": "P1626",
        "is_paper": true,
        "keywords": [
          "corpus creation",
          "benchmarking",
          "nlp datasets"
        ],
        "languages": [],
        "material": null,
        "paper_pdf": "https://aclanthology.org/2023.acl-long.2.pdf",
        "paper_type": "Main-Oral",
        "poster_pdf": "https://assets.underline.io/lecture/76227/poster_document/0299acb5c4aef0941dfe58944b78c8d4.pdf",
        "preview_image": "https://assets.underline.io/lecture/76227/poster/d7e40e3009397a8bb9a91901c7be9c15.png",
        "program": "Main",
        "similar_paper_ids": [],
        "slides_pdf": "https://assets.underline.io/lecture/76227/slideshow/e6eb4b7100fb906457e8b958e217ffe7.pdf",
        "title": "SafeConv: Explaining and Correcting Conversational Unsafe Behavior",
        "tldr": "One of the main challenges open-domain end-to-end dialogue systems, or chatbots, face is the prevalence of unsafe behavior, such as toxic languages and harmful suggestions. However, existing dialogue datasets do not provide enough annotation to explain and correct such unsafe behavior. In this work,...",
        "track": "Resources and Evaluation",
        "underline_id": 76227,
        "underline_url": "https://underline.io/events/395/sessions/15233/lecture/76227-safeconv-explaining-and-correcting-conversational-unsafe-behavior",
        "video_url": null
      }
    ],
    [
      "CODI_40",
      {
        "abstract": "(ACL Findings paper)Inspired by the curvature of space-time, we introduce Curved Contrastive Learning (CCL), a novel representation learning technique for learning the relative turn distance between utterance pairs in multi-turn dialogues. The resulting bi-encoder models can guide transformers as a response ranking model towards a goal in a zero-shot fashion by projecting the goal utterance and the corresponding reply candidates into a latent space. Here the cosine similarity indicates the distance/reachability of a candidate utterance toward the corresponding goal. Furthermore, we explore how these forward-entailing language representations can be utilized for assessing the likelihood of sequences by the entailment strength i.e. through the cosine similarity of its individual members (encoded separately) as an emergent property in the curved space. These non-local properties allow us to imagine the likelihood of future patterns in dialogues, specifically by ordering/identifying future goal utterances that are multiple turns away, given a dialogue context. As part of our analysis, we investigate characteristics that make conversations (un)plannable and find strong evidence of planning capability over multiple turns (in 61.56% over 3 turns) in conversations from the DailyDialog dataset. Finally, we show how we achieve higher efficiency in sequence modeling tasks compared to previous work thanks to our relativistic approach, where only the last utterance needs to be encoded and computed during inference.",
        "anthology_url": null,
        "authors": [
          "Justus-jonas Erker",
          "Stefan Schaffer",
          "Gerasimos Spanakis"
        ],
        "category": "Workshop",
        "demo_url": null,
        "display_track": null,
        "event_ids": [
          "CODI"
        ],
        "id": "CODI_40",
        "is_paper": true,
        "keywords": [],
        "languages": [],
        "material": null,
        "paper_pdf": null,
        "paper_type": "Extended abstract",
        "poster_pdf": null,
        "preview_image": null,
        "program": "Workshop",
        "similar_paper_ids": [],
        "slides_pdf": null,
        "title": "Imagination is All You Need! Curved Contrastive Learning for Abstract Sequence Modeling Utilized on Long Short-Term Dialogue Planning",
        "tldr": "(ACL Findings paper)Inspired by the curvature of space-time, we introduce Curved Contrastive Learning (CCL), a novel representation learning technique for learning the relative turn distance between utterance pairs in multi-turn dialogues. The resulting bi-encoder models can guide transformers as a ",
        "track": "4th Workshop on Computational Approaches to Discourse",
        "underline_id": null,
        "underline_url": null,
        "video_url": null
      }
    ],
    [
      "P2012",
      {
        "abstract": "Remarkable progress has been made on automated reasoning with natural text, by using Large Language Models (LLMs) and methods such as Chain-of-Thought prompting and Selection-Inference. These techniques search for proofs in the forward direction from axioms to the conclusion, which suffers from a combinatorial explosion of the search space, and thus high failure rates for problems requiring longer chains of reasoning. The classical automated reasoning literature has shown that reasoning in the backward direction (i.e. from intended conclusion to supporting axioms) is significantly more efficient at proof-finding. Importing this intuition into the LM setting, we develop a Backward Chaining algorithm, called LAMBADA, that decomposes reasoning into four sub-modules, that are simply implemented by few-shot prompted LLM inference. We show that LAMBADA achieves sizable accuracy boosts over state-of-the-art forward reasoning methods on two challenging logical reasoning datasets, particularly when deep and accurate proof chains are required.",
        "anthology_url": "https://aclanthology.org/2023.acl-long.361",
        "authors": [
          "Mehran Kazemi",
          "Najoung Kim",
          "Deepti Bhatia",
          "Xin Xu",
          "Deepak Ramachandran"
        ],
        "category": "Main-Poster",
        "demo_url": null,
        "display_track": "Large Language Models",
        "event_ids": [
          "poster-session-5_-large-language-models-(poster)"
        ],
        "id": "P2012",
        "is_paper": true,
        "keywords": [
          "prompting"
        ],
        "languages": [],
        "material": null,
        "paper_pdf": "https://aclanthology.org/2023.acl-long.361.pdf",
        "paper_type": "Main-Poster",
        "poster_pdf": "https://assets.underline.io/lecture/76658/poster_document/1d532f4c2fc851c73c2deabd6f8dffd2.pdf",
        "preview_image": "https://assets.underline.io/lecture/76658/poster/864cbe5da4b5ca41aa33e0f80c8205fc.png",
        "program": "Main",
        "similar_paper_ids": [],
        "slides_pdf": null,
        "title": "LAMBADA: Backward Chaining for Automated Reasoning in Natural Language",
        "tldr": "Remarkable progress has been made on automated reasoning with natural text, by using Large Language Models (LLMs) and methods such as Chain-of-Thought prompting and Selection-Inference. These techniques search for proofs in the forward direction from axioms to the conclusion, which suffers from a co...",
        "track": "Large Language Models",
        "underline_id": 76658,
        "underline_url": "https://underline.io/events/395/posters/15254/poster/76658-lambada-backward-chaining-for-automated-reasoning-in-natural-language",
        "video_url": null
      }
    ],
    [
      "SemEval_107",
      {
        "abstract": "This paper describes our system used for sub-task C (1 \\& 2) in Task 6: LegalEval: Understanding Legal Texts. We propose a three-level encoder-based classification architecture that works by fine-tuning a BERT-based pre-trained encoder, and post-processing the embeddings extracted from its last layers, using transformer encoder layers and RNNs. We run ablation studies on the same and analyze itsperformance. To extract the explanations for the predicted class we develop an explanation extraction algorithm, exploiting the idea of a model's occlusion sensitivity. We explored some training strategies with a detailed analysis of the dataset. Our system ranks 2nd (macro-F1 metric) for its sub-task C-1 and 7th (ROUGE-2 metric) for sub-task C-2.",
        "anthology_url": null,
        "authors": [
          "Nishchal Prasad",
          "Mohand Boughanem",
          "Taoufiq Dkaki"
        ],
        "category": "Workshop",
        "demo_url": null,
        "display_track": null,
        "event_ids": [
          "SemEval"
        ],
        "id": "SemEval_107",
        "is_paper": true,
        "keywords": [],
        "languages": [],
        "material": null,
        "paper_pdf": null,
        "paper_type": "Task 6: LegalEval: Understanding Legal Texts",
        "poster_pdf": null,
        "preview_image": null,
        "program": "Workshop",
        "similar_paper_ids": [],
        "slides_pdf": null,
        "title": "IRIT_IRIS_C at SemEval-2023 Task 6: A Multi-level Encoder-based Architecture for Judgement Prediction of Legal Cases and their Explanation",
        "tldr": "This paper describes our system used for sub-task C (1 \\& 2) in Task 6: LegalEval: Understanding Legal Texts. We propose a three-level encoder-based classification architecture that works by fine-tuning a BERT-based pre-trained encoder, and post-processing the embeddings extracted from its last laye",
        "track": "The 17th International Workshop on Semantic Evaluation (SemEval-2023)",
        "underline_id": null,
        "underline_url": null,
        "video_url": null
      }
    ]
  ]
}