[{"abstract":"Sentence embeddings induced with various transformer architectures encode much semantic and syntactic information in a distributed manner in a one-dimensional array. We investigate whether specific grammatical information can be accessed in these distributed representations. Using data from a task developed to test rule-like generalizations, our experiments on detecting subject-verb agreement yield several promising results. First, we show that while the usual sentence representations encoded as one-dimensional arrays do not easily support extraction of rule-like regularities, a two-dimensional  reshaping of these vectors allows various learning architectures to access such information. Next, we show that various architectures can detect patterns in these two-dimensional reshaped sentence embeddings and successfully learn a model based on smaller amounts of simpler training data, which performs well on more complex test data. This indicates that current sentence embeddings contain information that is regularly distributed, and which can be captured when the embeddings are reshaped into higher dimensional arrays. Our results cast light on representations produced by language models and help move towards developing few-shot learning approaches.","anthology_url":null,"authors":["Vivi Nastase","Paola Merlo"],"category":"Workshop","demo_url":null,"display_track":null,"event_ids":["RepL4NLP"],"id":"ACL_5","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":null,"paper_type":"N/A","poster_pdf":null,"preview_image":null,"program":"Workshop","similar_paper_ids":[],"slides_pdf":null,"title":"Grammatical information in BERT sentence embeddings as two-dimensional arrays","tldr":"Sentence embeddings induced with various transformer architectures encode much semantic and syntactic information in a distributed manner in a one-dimensional array. We investigate whether specific grammatical information can be accessed in these distributed representations. Using data from a task d","track":"The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)","underline_id":null,"underline_url":null,"video_url":null},{"abstract":"In this paper, we highlight our approach for the \"WASSA 2023 Shared-Task 1: Empathy Detection and Emotion Classification\". By accurately identifying emotions from textual sources of data, deep learning models can be trained to understand and interpret human emotions more effectively. The classification of emotions facilitates the creation of more emotionally intelligent systems that can better understand and respond to human emotions. We compared multiple transformer-based models for multi-label classification. Ensembling and oversampling were used to improve the performance of the system. A threshold-based voting mechanism performed on three models (Longformer, BERT, BigBird) yields the highest overall macro F1-score of 0.6605.","anthology_url":null,"authors":["Aditya Paranjape","Gaurav Kolhatkar","Yash Patwardhan","Omkar Gokhale","Shweta Dharmadhikari"],"category":"Workshop","demo_url":null,"display_track":null,"event_ids":["WASSA"],"id":"WASSA_108","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":null,"paper_type":"long","poster_pdf":null,"preview_image":null,"program":"Workshop","similar_paper_ids":[],"slides_pdf":null,"title":"Converge at WASSA 2023 Empathy, Emotion and Personality Shared Task: A Transformer-based Approach for Multi-Label Emotion Classification","tldr":"In this paper, we highlight our approach for the \"WASSA 2023 Shared-Task 1: Empathy Detection and Emotion Classification\". By accurately identifying emotions from textual sources of data, deep learning models can be trained to understand and interpret human emotions more effectively. The classificat","track":"The 13th Workshop on Computational Approaches to Subjectivity, Sentiment, & Social Media Analysis","underline_id":null,"underline_url":null,"video_url":null},{"abstract":"Maltese is a low-resource language of Arabic and Romance origins written in Latin script. We explore the impact of transliterating Maltese into Arabic script on a number of downstream tasks. We compare multiple transliteration pipelines ranging from simple one-to-one character maps to more sophisticated alternatives that explore multiple possibilities or make use of manual linguistic annotations. We show that the sophisticated systems are consistently better than simpler systems, quantitatively and qualitatively. We also show transliterating Maltese can be considered as an option to improve the cross-lingual transfer capabilities.","anthology_url":null,"authors":["Kurt Micallef","Fadhl Eryani","Nizar Habash","Houda Bouamor","Claudia Borg"],"category":"Workshop","demo_url":null,"display_track":null,"event_ids":["CAWL"],"id":"CAWL_14","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":null,"paper_type":"","poster_pdf":null,"preview_image":null,"program":"Workshop","similar_paper_ids":[],"slides_pdf":null,"title":"Exploring the Impact of Transliteration on NLP Performance for Low-Resource Languages: The Case of Maltese and Arabic","tldr":"Maltese is a low-resource language of Arabic and Romance origins written in Latin script. We explore the impact of transliterating Maltese into Arabic script on a number of downstream tasks. We compare multiple transliteration pipelines ranging from simple one-to-one character maps to more sophistic","track":"The Workshop on Computation and Written Language (CAWL)","underline_id":null,"underline_url":null,"video_url":null},{"abstract":"Pretrained language models leverage self-supervised learning to use large amounts of unlabeled text for learning contextual representations of sequences. However, in the domain of medical conversations, the availability of large, public datasets is limited due to issues of privacy and data management. In this paper, we study the effectiveness of dialog-aware pretraining objectives and multiphase training in using unlabeled data to improve LMs training for medical utterance classification. The objectives of pretraining for dialog awareness involve tasks that take into account the structure of conversations, including features such as turn-taking and the roles of speakers. The multiphase training process uses unannotated data in a sequence that prioritizes similarities and connections between different domains. We empirically evaluate these methods on conversational dialog classification tasks in the medical and counseling domains, and find that multiphase training can help achieve higher performance than standard pretraining or finetuning.","anthology_url":null,"authors":["Do June Min","Veronica Perez-Rosas","Rada Mihalcea"],"category":"Workshop","demo_url":null,"display_track":null,"event_ids":["Clinical-NLP"],"id":"ClinicalNLP_12","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":null,"paper_type":"N/A","poster_pdf":null,"preview_image":null,"program":"Workshop","similar_paper_ids":[],"slides_pdf":null,"title":"Navigating Data Scarcity: Pretraining for Medical Utterance Classification","tldr":"Pretrained language models leverage self-supervised learning to use large amounts of unlabeled text for learning contextual representations of sequences. However, in the domain of medical conversations, the availability of large, public datasets is limited due to issues of privacy and data managemen","track":"The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)","underline_id":null,"underline_url":null,"video_url":null},{"abstract":"Text Style Transfer (TST) evaluation is, in practice, inconsistent. Therefore, we conduct a meta-analysis on human and automated TST evaluation and experimentation that thoroughly examines existing literature in the field. The meta-analysis reveals a substantial standardization gap in human and automated evaluation. In addition, we also find a validation gap: only few automated metrics have been validated using human experiments. To this end, we thoroughly scrutinize both the standardization and validation gap and reveal the resulting pitfalls. This work also paves the way to close the standardization and validation gap in TST evaluation by calling out requirements to be met by future research.","anthology_url":null,"authors":["Phil Ostheimer","Mayank Kumar Nagda","Marius Kloft","Sophie Fellenz"],"category":"Workshop","demo_url":null,"display_track":null,"event_ids":["LAW"],"id":"LAW_F7","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":null,"paper_type":"","poster_pdf":null,"preview_image":null,"program":"Workshop","similar_paper_ids":[],"slides_pdf":null,"title":"A Call for Standardization and Validation of Text Style Transfer Evaluation","tldr":"Text Style Transfer (TST) evaluation is, in practice, inconsistent. Therefore, we conduct a meta-analysis on human and automated TST evaluation and experimentation that thoroughly examines existing literature in the field. The meta-analysis reveals a substantial standardization gap in human and auto","track":"The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\ @ ACL 2023","underline_id":null,"underline_url":null,"video_url":null},{"abstract":"This paper describes the system and experimental results of an ensemble-based approach tomultilingual framing detection for the submission of the ACCEPT team to the SemEval-2023 Task 3 on Framing Detection (Subtask 2). The approach is based on an ensemble that combines three different methods: a classifier based on large language models, a classifier based on static word embeddings, and an approach that uses external commonsense knowledge graphs, in particular, ConceptNet. The results of the three classification heads are aggregated into an overall prediction for each frame class.Our best submission yielded a micro F1-score of 50.69\\% (rank 10) and a macro F1-score of 50.20\\% (rank 3) for English articles. Our experimental results show that static word embeddings and knowledge graphs are useful components for frame detection, while the ensemble of all three methods combines the strengths of our three proposed methods. Through system ablations, we show that the commonsenseguided knowledge graphs are the outperforming method for many languages.","anthology_url":null,"authors":["Philipp Heinisch","Moritz Plenz","Anette Frank","Philipp Cimiano"],"category":"Workshop","demo_url":null,"display_track":null,"event_ids":["SemEval"],"id":"SemEval_205","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":null,"paper_type":"Task 3: Detecting the Category, the Framing, and the Persuasion Techniques in Online News in a Multi-lingual Setup","poster_pdf":null,"preview_image":null,"program":"Workshop","similar_paper_ids":[],"slides_pdf":null,"title":"ACCEPT at SemEval-2023 Task 3: An Ensemble-based Approach to Multilingual Framing Detection","tldr":"This paper describes the system and experimental results of an ensemble-based approach tomultilingual framing detection for the submission of the ACCEPT team to the SemEval-2023 Task 3 on Framing Detection (Subtask 2). The approach is based on an ensemble that combines three different methods: a cla","track":"The 17th International Workshop on Semantic Evaluation (SemEval-2023)","underline_id":null,"underline_url":null,"video_url":null},{"abstract":"In this paper, we present the KU x Upstage team's submission for the Special Task on Formality Control on Spoken Language Translation, which involves translating English into four languages with diverse grammatical formality markers.  Our methodology comprises two primary components: 1) a language-specific data-driven approach, and 2) the generation of synthetic data through the employment of large-scale language models and empirically-grounded prompt engineering. By adapting methodologies and models to accommodate the unique linguistic properties of each language, we observe a notable enhancement in performance relative to the baseline, substantiating the heightened efficacy of data-driven approaches. Moreover, our devised prompt engineering strategy yields superior synthetic translation instances.","anthology_url":null,"authors":["Seugnjun Lee","Hyeonseok Moon","Chanjun Park","Heuiseok Lim"],"category":"Workshop","demo_url":null,"display_track":null,"event_ids":["IWSLT"],"id":"IWSLT_47","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":null,"paper_type":"long","poster_pdf":null,"preview_image":null,"program":"Workshop","similar_paper_ids":[],"slides_pdf":null,"title":"Improving Formality-Sensitive Machine Translation Using Data-Centric Approaches and Prompt Engineering","tldr":"In this paper, we present the KU x Upstage team's submission for the Special Task on Formality Control on Spoken Language Translation, which involves translating English into four languages with diverse grammatical formality markers.  Our methodology comprises two primary components: 1) a language-s","track":"The 20th International Conference on Spoken Language Translation","underline_id":null,"underline_url":null,"video_url":null},{"abstract":"This study describes the model design of the NCUEE-NLP system for the SemEval-2023 NLI4CT task that focuses on multi-evidence natural language inference for clinical trial data. We use the LinkBERT transformer in the biomedical domain (denoted as BioLinkBERT) as our main system architecture. First, a set of sentences in clinical trial reports is extracted as evidence for premise-statement inference. This identified evidence is then used to determine the inference relation (i.e., entailment or contradiction). Finally, a soft voting ensemble mechanism is applied to enhance the system performance. For Subtask 1 on textual entailment, our best submission had an F1-score of 0.7091, ranking sixth among all 30 participating teams. For Subtask 2 on evidence retrieval, our best result obtained an F1-score of 0.7940, ranking ninth of 19 submissions.","anthology_url":null,"authors":["Chao-Yi Chen","Kao-Yuan Tien","Yuan-Hao Cheng","Lung-Hao Lee"],"category":"Workshop","demo_url":null,"display_track":null,"event_ids":["SemEval"],"id":"SemEval_120","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":null,"paper_type":"Task 7: Multi-Evidence Natural Language Inference for Clinical Trial Data","poster_pdf":null,"preview_image":null,"program":"Workshop","similar_paper_ids":[],"slides_pdf":null,"title":"NCUEE-NLP at SemEval-2023 Task 7: Ensemble Biomedical LinkBERT Transformers in Multi-evidence Natural Language Inference for Clinical Trial Data","tldr":"This study describes the model design of the NCUEE-NLP system for the SemEval-2023 NLI4CT task that focuses on multi-evidence natural language inference for clinical trial data. We use the LinkBERT transformer in the biomedical domain (denoted as BioLinkBERT) as our main system architecture. First, ","track":"The 17th International Workshop on Semantic Evaluation (SemEval-2023)","underline_id":null,"underline_url":null,"video_url":null},{"abstract":"This paper describes SinaAI's participation in SemEval-2023 Task 3, which involves detecting propaganda in news articles across multiple languages. The task comprises three sub-tasks: (i) genre detection, (ii) news framing,and (iii) persuasion technique identification.The employed dataset includes news articles in nine languages and domains, including English, French, Italian, German, Polish, Russian, Georgian, Greek, and Spanish, with labeled instances of news framing, genre, and persuasion techniques. Our approach combines fine-tuning  multilingual language models such as XLM, LaBSE, and mBERT with data augmentation techniques. Our experimental results show that XLM outperforms other models in terms of F1-Micro in and F1-Macro, and the ensemble of XLM and LaBSE achieved the best performance. Our study highlights the effectiveness of multilingual sentence embedding models in multilingual propaganda detection. Our models achieved highest score for two languages (greek and italy) in sub-task 1 and one language (Russian) for sub-task 2.","anthology_url":null,"authors":["Aryan Sadeghi","Reza Alipour","Kamyar Taeb","Parimehr Morassafar","Nima Salemahim","Ehsaneddin Asgari"],"category":"Workshop","demo_url":null,"display_track":null,"event_ids":["SemEval"],"id":"SemEval_330","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":null,"paper_type":"Task 3: Detecting the Category, the Framing, and the Persuasion Techniques in Online News in a Multi-lingual Setup","poster_pdf":null,"preview_image":null,"program":"Workshop","similar_paper_ids":[],"slides_pdf":null,"title":"SinaAI at SemEval-2023 Task 3: A Multilingual Transformer Language Model-based Approach for the Detection of News Genre, Framing and Persuasion Techniques","tldr":"This paper describes SinaAI's participation in SemEval-2023 Task 3, which involves detecting propaganda in news articles across multiple languages. The task comprises three sub-tasks: (i) genre detection, (ii) news framing,and (iii) persuasion technique identification.The employed dataset includes n","track":"The 17th International Workshop on Semantic Evaluation (SemEval-2023)","underline_id":null,"underline_url":null,"video_url":null},{"abstract":"End-to-end automatic speech translation (AST) relies on data that combines audio inputs with text translation outputs. Previous work used existing large parallel corpora of transcriptions and translations in a knowledge distillation (KD) setup to distill a neural machine translation (NMT) into an AST student model. While KD allows using larger pretrained models, the reliance of previous KD approaches on manual audio transcripts in the data pipeline restricts the applicability of this framework to AST. We present an imitation learning approach where a teacher NMT system corrects the errors of an AST student without relying on manual transcripts. We show that the NMT teacher can recover from errors in automatic transcriptions and is able to correct erroneous translations of the AST student, leading to improvements of about 4 BLEU points over the standard AST end-to-end baseline on the English-German CoVoST-2 and MuST-C datasets, respectively. Code and data are publicly available: https://github.com/HubReb/imitkd_ast/releases/tag/v1.1","anthology_url":null,"authors":["Rebekka Hubert","Artem Sokolov","Stefan Riezler"],"category":"Workshop","demo_url":null,"display_track":null,"event_ids":["IWSLT"],"id":"IWSLT_4","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":null,"paper_type":"long","poster_pdf":null,"preview_image":null,"program":"Workshop","similar_paper_ids":[],"slides_pdf":null,"title":"Improving End-to-End Speech Translation by Imitation-Based Knowledge Distillation with Synthetic Transcripts","tldr":"End-to-end automatic speech translation (AST) relies on data that combines audio inputs with text translation outputs. Previous work used existing large parallel corpora of transcriptions and translations in a knowledge distillation (KD) setup to distill a neural machine translation (NMT) into an AS","track":"The 20th International Conference on Spoken Language Translation","underline_id":null,"underline_url":null,"video_url":null},{"abstract":"(ACL Findings paper)Inspired by the curvature of space-time, we introduce Curved Contrastive Learning (CCL), a novel representation learning technique for learning the relative turn distance between utterance pairs in multi-turn dialogues. The resulting bi-encoder models can guide transformers as a response ranking model towards a goal in a zero-shot fashion by projecting the goal utterance and the corresponding reply candidates into a latent space. Here the cosine similarity indicates the distance/reachability of a candidate utterance toward the corresponding goal. Furthermore, we explore how these forward-entailing language representations can be utilized for assessing the likelihood of sequences by the entailment strength i.e. through the cosine similarity of its individual members (encoded separately) as an emergent property in the curved space. These non-local properties allow us to imagine the likelihood of future patterns in dialogues, specifically by ordering/identifying future goal utterances that are multiple turns away, given a dialogue context. As part of our analysis, we investigate characteristics that make conversations (un)plannable and find strong evidence of planning capability over multiple turns (in 61.56% over 3 turns) in conversations from the DailyDialog dataset. Finally, we show how we achieve higher efficiency in sequence modeling tasks compared to previous work thanks to our relativistic approach, where only the last utterance needs to be encoded and computed during inference.","anthology_url":null,"authors":["Justus-jonas Erker","Stefan Schaffer","Gerasimos Spanakis"],"category":"Workshop","demo_url":null,"display_track":null,"event_ids":["CODI"],"id":"CODI_40","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":null,"paper_type":"Extended abstract","poster_pdf":null,"preview_image":null,"program":"Workshop","similar_paper_ids":[],"slides_pdf":null,"title":"Imagination is All You Need! Curved Contrastive Learning for Abstract Sequence Modeling Utilized on Long Short-Term Dialogue Planning","tldr":"(ACL Findings paper)Inspired by the curvature of space-time, we introduce Curved Contrastive Learning (CCL), a novel representation learning technique for learning the relative turn distance between utterance pairs in multi-turn dialogues. The resulting bi-encoder models can guide transformers as a ","track":"4th Workshop on Computational Approaches to Discourse","underline_id":null,"underline_url":null,"video_url":null},{"abstract":"This paper describes our system used for sub-task C (1 \\& 2) in Task 6: LegalEval: Understanding Legal Texts. We propose a three-level encoder-based classification architecture that works by fine-tuning a BERT-based pre-trained encoder, and post-processing the embeddings extracted from its last layers, using transformer encoder layers and RNNs. We run ablation studies on the same and analyze itsperformance. To extract the explanations for the predicted class we develop an explanation extraction algorithm, exploiting the idea of a model's occlusion sensitivity. We explored some training strategies with a detailed analysis of the dataset. Our system ranks 2nd (macro-F1 metric) for its sub-task C-1 and 7th (ROUGE-2 metric) for sub-task C-2.","anthology_url":null,"authors":["Nishchal Prasad","Mohand Boughanem","Taoufiq Dkaki"],"category":"Workshop","demo_url":null,"display_track":null,"event_ids":["SemEval"],"id":"SemEval_107","is_paper":true,"keywords":[],"languages":[],"material":null,"paper_pdf":null,"paper_type":"Task 6: LegalEval: Understanding Legal Texts","poster_pdf":null,"preview_image":null,"program":"Workshop","similar_paper_ids":[],"slides_pdf":null,"title":"IRIT_IRIS_C at SemEval-2023 Task 6: A Multi-level Encoder-based Architecture for Judgement Prediction of Legal Cases and their Explanation","tldr":"This paper describes our system used for sub-task C (1 \\& 2) in Task 6: LegalEval: Understanding Legal Texts. We propose a three-level encoder-based classification architecture that works by fine-tuning a BERT-based pre-trained encoder, and post-processing the embeddings extracted from its last laye","track":"The 17th International Workshop on Semantic Evaluation (SemEval-2023)","underline_id":null,"underline_url":null,"video_url":null}]
